<!DOCTYPE html><html lang="en"> <head><meta charset="utf-8"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><meta name="viewport" content="width=device-width"><meta name="generator" content="Astro v5.10.1"><title>Attention and its gradient - Academic Homepage</title><link rel="stylesheet" href="https://fred-wang.github.io/MathFonts/NewComputerModern/mathfonts.css"><link rel="stylesheet" href="/_astro/blog.CJoC8E46.css"></head> <body class="blog-post"> <!-- SEO Meta Tags --><title>Research Article - Xiaotian Han | Academic Insights</title><meta name="title" content="Research Article - Xiaotian Han | Academic Insights"><meta name="description" content="Academic research article by Xiaotian Han on machine learning and large language models."><meta name="keywords" content="research article, machine learning, large language models, academic research, LLM"><meta name="author" content="Xiaotian Han"><meta name="robots" content="index, follow"><meta name="language" content="en"><meta name="revisit-after" content="7 days"><!-- Canonical URL --><link rel="canonical" href="https://ahxt.github.io/blog/2024-10-20-attention/"><!-- Open Graph / Facebook --><meta property="og:type" content="article"><meta property="og:url" content="https://ahxt.github.io/blog/2024-10-20-attention/"><meta property="og:title" content="Research Article - Xiaotian Han | Academic Insights"><meta property="og:description" content="Academic research article by Xiaotian Han on machine learning and large language models."><meta property="og:image" content="https://ahxt.github.io//xt.png"><meta property="og:image:alt" content="Xiaotian Han - Profile Photo"><meta property="og:site_name" content="Xiaotian Han - Academic Homepage"><meta property="og:locale" content="en_US"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://ahxt.github.io/blog/2024-10-20-attention/"><meta property="twitter:title" content="Research Article - Xiaotian Han | Academic Insights"><meta property="twitter:description" content="Academic research article by Xiaotian Han on machine learning and large language models."><meta property="twitter:image" content="https://ahxt.github.io//xt.png"><meta property="twitter:image:alt" content="Xiaotian Han - Profile Photo"><meta property="twitter:creator" content="@XiaotianHan1"><!-- Additional Meta Tags for Academic Site --><meta name="theme-color" content="#1e40af"><meta name="msapplication-TileColor" content="#1e40af"><!-- Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","name":"Xiaotian Han","headline":"Research Article - Xiaotian Han | Academic Insights","jobTitle":"Assistant Professor of Computer Science","worksFor":{"@type":"Organization","name":"Case Western Reserve University","url":"https://case.edu"},"alumniOf":{"@type":"Organization","name":"Texas A&M University"},"knowsAbout":["Machine Learning","Large Language Models","Computer Science","Artificial Intelligence","LLMs"],"url":"https://ahxt.github.io/blog/2024-10-20-attention/","image":"https://ahxt.github.io//xt.png","sameAs":["https://scholar.google.com/citations?hl=en&user=Uromx98AAAAJ&view_op=list_works&sortby=pubdate","https://x.com/XiaotianHan1","https://bsky.app/profile/xhan2.bsky.social","https://github.com/ahxt"],"author":{"@type":"Person","name":"Xiaotian Han"},"publisher":{"@type":"Person","name":"Xiaotian Han"},"datePublished":"2025-06-28T03:59:49.014Z","dateModified":"2025-06-28T03:59:49.014Z","description":"Academic research article by Xiaotian Han on machine learning and large language models."}</script><!-- Preconnect to external domains for performance --><link rel="preconnect" href="https://scholar.google.com"><link rel="preconnect" href="https://github.com"><link rel="dns-prefetch" href="https://x.com"><link rel="dns-prefetch" href="https://bsky.app"><header> <nav> <div class="logo-container"> <h2 class="logo"> <a href="/" class="logo-link"> <span class="logo-text">Xiaotian Han</span> </a> </h2> </div> <div class="internal-links"> <a href="/" class="nav-link "> <span class="nav-text">About</span> <div class="nav-indicator"></div> </a> <a href="/blog" class="nav-link active"> <span class="nav-text">Blog</span> <div class="nav-indicator"></div> </a> </div> </nav> </header> <div class="footer" hidden="hidden"> <div class="center"> <a><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=m&d=91g_Uih-7fadH9madF_Vex1LQXOVlduL5aeBBSKXgXA&co=2d78ad&ct=ffffff"></a> </div> </div> <main> <div> <article> <div class="post-header"> <h4>Attention and its gradient</h4> <div class="post-meta"> <div class="date"> October 19, 2024 </div> <div class="author">by Xiaotian Han</div> </div> </div> <div class="content"> <div>
  <div>1.Background</div>
  <div>2.Attention</div>
  <div>3.Backprop Derivation</div>
  <div>4.PyTorch implementation</div>
  <h2>1. Background</h2>
  <p>Until now, the official flashattn implementation does not support bias term. Flexattention in <code>torch</code> is trying to support the bias term now. In this blog, I will show how to implement a minimal flashattn with trainable bias term.</p>
  <h2>2. Attention</h2>
  <p>The attention with gradient-enabled bias term is defined as:</p>
  <math display="block"><mrow><mi>𝑶</mi><mo>=</mo><mrow><mrow><mtext>softmax</mtext><mrow><mo>(</mo><mrow><mfrac><msup><mrow><mi>𝑸</mi><mi>𝑲</mi></mrow><mi>𝑇</mi></msup><msqrt><mi>𝑑</mi></msqrt></mfrac><mo>+</mo><mi>𝑩</mi></mrow><mo>)</mo></mrow></mrow><mi>𝑽</mi></mrow></mrow></math>
  <p>where</p>
  <ul>
    <li><span><math><mi>𝑩</mi></math></span> is the bias term and the shape is <span><math><mrow><mo>(</mo><mrow><mrow><mi>𝑛</mi><mo>,</mo></mrow><mrow><mi>ℎ</mi><mo>,</mo></mrow><mrow><mi>𝑙</mi><mo>,</mo></mrow><mi>𝑙</mi></mrow><mo>)</mo></mrow></math></span></li>
    <li>The shape of <span><math><mrow><mrow><mi>𝑸</mi><mo>,</mo></mrow><mrow><mi>𝑲</mi><mo>,</mo></mrow><mi>𝑽</mi></mrow></math></span> is <span><math><mrow><mo>(</mo><mrow><mrow><mi>𝑛</mi><mo>,</mo></mrow><mrow><mi>ℎ</mi><mo>,</mo></mrow><mrow><mi>𝑙</mi><mo>,</mo></mrow><mi>𝑑</mi></mrow><mo>)</mo></mrow></math></span></li>
    <li><span><math><mi>𝑛</mi></math></span> is batch size, <span><math><mi>ℎ</mi></math></span> is heads number, <span><math><mi>𝑙</mi></math></span> is sequence length, <span><math><mi>𝑑</mi></math></span> is hidden dimension.</li>
  </ul>
  <p>The gradient of <span><math><mi>𝑩</mi></math></span> is accumulated during the training process.</p>
  <h2>3. Backprop Derivation</h2>
  <p>Let</p>
  <math display="block"><mtable><mtr><mtd class="mathyml-align-right"><mi>𝑺</mi></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><msup><mrow><mi>𝑸</mi><mi>𝑲</mi></mrow><mi>𝑇</mi></msup><msqrt><mi>𝑑</mi></msqrt></mfrac><mo>+</mo><mi>𝑩</mi></mtd></mtr><mtr><mtd class="mathyml-align-right"><mi>𝑨</mi></mtd><mtd class="mathyml-align-left"><mo>=</mo><mrow><mtext>softmax</mtext><mrow><mo>(</mo><mi>𝑺</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mtext>softmax</mtext><mrow><mo>(</mo><mrow><mfrac><msup><mrow><mi>𝑸</mi><mi>𝑲</mi></mrow><mi>𝑇</mi></msup><msqrt><mi>𝑑</mi></msqrt></mfrac><mo>+</mo><mi>𝑩</mi></mrow><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd class="mathyml-align-right"><mi>𝑶</mi></mtd><mtd class="mathyml-align-left"><mo>=</mo><mrow><mi>𝑨</mi><mi>𝑽</mi></mrow><mo>=</mo><mrow><mtext>softmax</mtext><mrow><mo>(</mo><mi>𝑺</mi><mo>)</mo></mrow></mrow><mi>𝑽</mi><mo>=</mo><mrow><mtext>softmax</mtext><mrow><mo>(</mo><mrow><mfrac><msup><mrow><mi>𝑸</mi><mi>𝑲</mi></mrow><mi>𝑇</mi></msup><msqrt><mi>𝑑</mi></msqrt></mfrac><mo>+</mo><mi>𝑩</mi></mrow><mo>)</mo></mrow></mrow><mi>𝑽</mi></mtd></mtr></mtable></math>
  <p>We already have the gradient of <span><math><mi>𝑶</mi></math></span> is</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑶</mi></mrow></mfrac><mspace width="1em"></mspace><mrow><mo>(</mo><mrow><mo>[</mo><mrow><mrow><mi>𝑛</mi><mo>,</mo></mrow><mrow><mi>ℎ</mi><mo>,</mo></mrow><mrow><mi>𝑙</mi><mo>,</mo></mrow><mi>𝑑</mi></mrow><mo>]</mo></mrow><mo>)</mo></mrow><mo>.</mo></mrow></math>
  <div>In the following, we think of each <span><math><mrow><mo>(</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi></mrow><mo>)</mo></mrow></math></span> slice as a separate matrix multiply.</div>
  <h3>3.1. Gradient of <span><math><mi>𝑽</mi></math></span> and <span><math><mi>𝑨</mi></math></span></h3>
  <p>Since</p>
  <math display="block"><mrow><mi>𝑶</mi><mo>=</mo><mrow><mi>𝑨</mi><mi>𝑽</mi></mrow><mspace width="1em"></mspace><mrow><mo>(</mo><mrow><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑑</mi></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi></mrow><mo>]</mo></mrow><mo>×</mo><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑑</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></math>
  <p>, we get</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑨</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑶</mi></mrow></mfrac><mtext> bmm </mtext><mrow><mo>(</mo><msup><mi>𝑽</mi><mi>𝑇</mi></msup><mo>)</mo></mrow><mo>,</mo><mspace width="1em"></mspace><mrow><mo>(</mo><mrow><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi></mrow><mo>]</mo></mrow><mo>×</mo><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑑</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></math><math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑽</mi></mrow></mfrac><mo>=</mo><msup><mi>𝑨</mi><mi>𝑇</mi></msup><mtext> bmm </mtext><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑶</mi></mrow></mfrac><mo>,</mo></mrow><mspace width="1em"></mspace><mrow><mo>(</mo><mrow><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑑</mi></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi></mrow><mo>]</mo></mrow><mo>×</mo><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑑</mi></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow></math>
  <h3>3.2. Gradient of <span><math><mi>𝑺</mi></math></span></h3>
  <p>It is easy to get the gradient of <span><math><mi>𝑺</mi></math></span> based on chain rule:</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><msub><mrow><mo>(</mo><mrow><mo>∂</mo><mi>𝑺</mi></mrow><mo>)</mo></mrow><mrow><mi>𝑖</mi><mi>𝑗</mi><mi>𝑘</mi><mi>𝑙</mi></mrow></msub></mfrac><mo>=</mo><munder displaystyle="true"><mo>∑</mo><mrow><mi>𝑚</mi><mo>,</mo><mi>𝑛</mi></mrow></munder><mfrac><mrow><mo>∂</mo><msub><mi>𝑨</mi><mrow><mi>𝑖</mi><mi>𝑗</mi><mi>𝑚</mi><mi>𝑛</mi></mrow></msub></mrow><mrow><mo>∂</mo><msub><mi>𝑺</mi><mrow><mi>𝑖</mi><mi>𝑗</mi><mi>𝑘</mi><mi>𝑙</mi></mrow></msub></mrow></mfrac><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><msub><mi>𝑨</mi><mrow><mi>𝑖</mi><mi>𝑗</mi><mi>𝑚</mi><mi>𝑛</mi></mrow></msub></mrow></mfrac></mrow></math>
  <p>where <span><math><mfrac><mrow><mo>∂</mo><mi>𝑨</mi></mrow><mrow><mo>∂</mo><mi>𝑺</mi></mrow></mfrac></math></span> is the Jacobian of softmax function and has size <span><math><mrow><mo>(</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi></mrow><mo>)</mo></mrow></math></span>. <span><math><mrow><mrow><mi>𝑖</mi><mo>,</mo></mrow><mrow><mi>𝑗</mi><mo>,</mo></mrow><mrow><mi>𝑘</mi><mo>,</mo></mrow><mi>𝑙</mi></mrow></math></span>: Indices of the target tensor <span><math><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑺</mi></mrow></mfrac></math></span>. <span><math><mrow><mrow><mi>𝑚</mi><mo>,</mo></mrow><mi>𝑛</mi></mrow></math></span>: Summation indices, specifying contraction over these dimensions. The <span><math><msub><mo>∑</mo><mrow><mrow><mi>𝑚</mi><mo>,</mo></mrow><mi>𝑛</mi></mrow></msub></math></span> explicitly indicates summation over the indices <span><math><mi>𝑚</mi></math></span> and <span><math><mi>𝑛</mi></math></span>.</p>
  <p>For efficiency, we can rewrite the above equation as:</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑺</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑨</mi></mrow></mfrac><mo>∗</mo><mrow><mo>(</mo><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑨</mi></mrow></mfrac><mo>−</mo><mrow><mo>(</mo><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑨</mi></mrow></mfrac><mo>⋅</mo><msup><mi>𝑨</mi><mi>𝑇</mi></msup></mrow><mo>)</mo></mrow><mn>𝟏</mn></mrow><mo>)</mo></mrow><mspace width="1em"></mspace><mrow><mo>(</mo><mrow><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi></mrow><mo>]</mo></mrow><mo>∗</mo><mrow><mo>(</mo><mrow><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi></mrow><mo>]</mo></mrow><mtext> bmm </mtext><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mi>𝑙</mi></mrow><mo>]</mo></mrow><mo>⋅</mo><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></math>
  <p>where <span><math><mrow><mn>𝟏</mn><mo>∈</mo><mrow><mo>[</mo><mrow><mi>𝑛</mi><mo>,</mo><mi>ℎ</mi><mo>,</mo><mi>𝑙</mi><mo>,</mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow></math></span>, summation vector to normalize contributions.</p>
  <h3>3.3. Gradient of <span><math><mi>𝑩</mi></math></span></h3>
  <p>The gradient of <span><math><mi>𝑩</mi></math></span> is the same as the gradient of <span><math><mi>𝑺</mi></math></span>, which is:</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑩</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑺</mi></mrow></mfrac></mrow></math>
  <h3>3.4. Gradient of <span><math><mi>𝑸</mi></math></span>, <span><math><mi>𝑲</mi></math></span></h3>
  <p>The gradient of <span><math><mi>𝑸</mi></math></span> and <span><math><mi>𝑲</mi></math></span> is:</p>
  <math display="block"><mtable><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑸</mi></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑺</mi></mrow></mfrac><mo>⋅</mo><mi>𝑲</mi></mtd></mtr><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑲</mi></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑺</mi></mrow></mfrac><mo>⋅</mo><mi>𝑸</mi></mtd></mtr></mtable></math>
  <h3>3.5. All gradients</h3>
  <math display="block"><mtable><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑸</mi></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑺</mi></mrow></mfrac><mo>⋅</mo><mi>𝑲</mi></mtd></mtr><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑲</mi></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑺</mi></mrow></mfrac><mo>⋅</mo><mi>𝑸</mi></mtd></mtr><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑽</mi></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><msup><mi>𝑨</mi><mi>𝑇</mi></msup><mtext> bmm </mtext><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑶</mi></mrow></mfrac></mtd></mtr><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑨</mi></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑶</mi></mrow></mfrac><mtext> bmm </mtext><mrow><mo>(</mo><msup><mi>𝑽</mi><mi>𝑇</mi></msup><mo>)</mo></mrow></mtd></mtr><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑺</mi></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑨</mi></mrow></mfrac><mo>∗</mo><mrow><mo>(</mo><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑨</mi></mrow></mfrac><mo>−</mo><mrow><mo>(</mo><mrow><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑨</mi></mrow></mfrac><mo>⋅</mo><msup><mi>𝑨</mi><mi>𝑇</mi></msup></mrow><mo>)</mo></mrow><mn>𝟏</mn></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑩</mi></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>ℒ</mi></mrow><mrow><mo>∂</mo><mi>𝑺</mi></mrow></mfrac></mtd></mtr></mtable></math>
  <h2>4. PyTorch implementation</h2>
  <pre>import torch<br><br>def forward(Q, K, V, B, d):<br>    S = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d, dtype=torch.float32)) + B<br>    A = torch.softmax(S, dim=-1)<br>    O = torch.matmul(A, V)<br>    return O, A, S<br><br>@torch.no_grad<br>def compute_gradients(Q, K, V, B, d, dO):<br>    # Compute forward pass<br>    S = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d, dtype=torch.float32)) + B<br>    A = torch.softmax(S, dim=-1)<br>    O = torch.matmul(A, V)<br><br>    # Gradient of V and A<br>    dA = torch.matmul(dO, V.transpose(-2, -1))<br>    dV = torch.matmul(A.transpose(-2, -1), dO)<br><br>    # Gradient of S using Jacobian-vector product (JVP)<br>    dS = dA * A - (A * dA).sum(dim=-1, keepdim=True) * A<br>    # dS = dA * A - torch.matmul(dA * A, A.transpose(-2, -1))<br><br>    # Gradient of B (same as dS)<br>    dB = dS.clone()<br><br>    # Gradient of Q and K<br>    dQ = torch.matmul(dS, K) / torch.sqrt(torch.tensor(d, dtype=torch.float32))<br>    dK = torch.matmul(dS.transpose(-2, -1), Q) / torch.sqrt(torch.tensor(d, dtype=torch.float32))<br><br>    return dQ, dK, dV, dB<br><br><br># Example usage<br>n, h, l, d = 2, 4, 8, 16<br>torch.manual_seed(0)<br>Q = torch.randn(n, h, l, d, requires_grad=True)<br>K = torch.randn(n, h, l, d, requires_grad=True)<br>V = torch.randn(n, h, l, d, requires_grad=True)<br>B = torch.randn(n, h, l, l, requires_grad=True)<br>dO = torch.randn(n, h, l, d)<br><br>O, A, S = forward(Q, K, V, B, d)<br>dQ, dK, dV, dB = compute_gradients(Q, K, V, B, d, dO)<br><br># Verify correctness with autograd<br>O.backward(dO, retain_graph=True)<br><br><br><br><br>print( V.grad[0][0][0])<br>print( dV[0][0][0]  )<br><br>print( B.grad[0][0][0])<br>print( dB[0][0][0]  )<br><br>print( Q.grad[0][0][0])<br>print( dQ[0][0][0]  )<br><br><br><br>assert torch.allclose(V.grad, dV, atol=1e-5), "dV mismatch"<br>assert torch.allclose(B.grad, dB, atol=1e-5), "dB mismatch"<br>assert torch.allclose(Q.grad, dQ, atol=1e-5), "dQ mismatch"<br>assert torch.allclose(K.grad, dK, atol=1e-5), "dK mismatch"<br><br><br>print("Autograd verification passed.")<br><br>print("O:", O.shape)<br>print("dQ:", dQ.shape)<br>print("dK:", dK.shape)<br>print("dV:", dV.shape)<br>print("dB:", dB.shape)</pre>
  <p>Output:</p>
  <pre>tensor([-0.9583, -0.7990, -0.7401,  0.4045, -1.1326, -0.8535,  0.9846,  0.8070,<br>        -0.6478, -0.0538,  0.6266,  1.0380, -0.9200,  0.5653,  0.9200, -0.0638])<br>tensor([-0.9583, -0.7990, -0.7401,  0.4045, -1.1326, -0.8535,  0.9846,  0.8070,<br>        -0.6478, -0.0538,  0.6266,  1.0380, -0.9200,  0.5653,  0.9200, -0.0638])<br>tensor([-8.4880e-02, -6.7330e-01, -5.2291e-04,  3.3246e-02, -2.7012e-02,<br>         5.0888e-01,  2.4558e-01, -1.9837e-03])<br>tensor([-8.4880e-02, -6.7330e-01, -5.2293e-04,  3.3246e-02, -2.7012e-02,<br>         5.0888e-01,  2.4558e-01, -1.9838e-03])<br>tensor([-0.1274, -0.2580,  0.2316,  0.1266, -0.3056,  0.0579, -0.2824,  0.2191,<br>        -0.0199,  0.2176, -0.0755, -0.1700,  0.1564,  0.2221, -0.0909,  0.0172])<br>tensor([-0.1274, -0.2580,  0.2316,  0.1266, -0.3056,  0.0579, -0.2824,  0.2191,<br>        -0.0199,  0.2176, -0.0755, -0.1700,  0.1564,  0.2221, -0.0909,  0.0172])<br>Autograd verification passed.<br>O: torch.Size([2, 4, 8, 16])<br>dQ: torch.Size([2, 4, 8, 16])<br>dK: torch.Size([2, 4, 8, 16])<br>dV: torch.Size([2, 4, 8, 16])<br>dB: torch.Size([2, 4, 8, 8])</pre>
</div>
 </div> <footer> <a href="/blog" class="back-link">← Back to Blog</a> </footer> </article> </div> </main> </body></html>