<!DOCTYPE html><html lang="en"> <head><meta charset="utf-8"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><meta name="viewport" content="width=device-width"><meta name="generator" content="Astro v5.10.1"><title>Reproduce the inference time scaling exp - Academic Homepage</title><link rel="stylesheet" href="https://fred-wang.github.io/MathFonts/NewComputerModern/mathfonts.css"><link rel="stylesheet" href="/_astro/blog.CJoC8E46.css"></head> <body class="blog-post"> <!-- SEO Meta Tags --><title>Research Article - Xiaotian Han | Academic Insights</title><meta name="title" content="Research Article - Xiaotian Han | Academic Insights"><meta name="description" content="Academic research article by Xiaotian Han on machine learning and large language models."><meta name="keywords" content="research article, machine learning, large language models, academic research, LLM"><meta name="author" content="Xiaotian Han"><meta name="robots" content="index, follow"><meta name="language" content="en"><meta name="revisit-after" content="7 days"><!-- Canonical URL --><link rel="canonical" href="https://ahxt.github.io/blog/2024-12-30-inference-time-scaling-exp/"><!-- Open Graph / Facebook --><meta property="og:type" content="article"><meta property="og:url" content="https://ahxt.github.io/blog/2024-12-30-inference-time-scaling-exp/"><meta property="og:title" content="Research Article - Xiaotian Han | Academic Insights"><meta property="og:description" content="Academic research article by Xiaotian Han on machine learning and large language models."><meta property="og:image" content="https://ahxt.github.io//xt.png"><meta property="og:image:alt" content="Xiaotian Han - Profile Photo"><meta property="og:site_name" content="Xiaotian Han - Academic Homepage"><meta property="og:locale" content="en_US"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://ahxt.github.io/blog/2024-12-30-inference-time-scaling-exp/"><meta property="twitter:title" content="Research Article - Xiaotian Han | Academic Insights"><meta property="twitter:description" content="Academic research article by Xiaotian Han on machine learning and large language models."><meta property="twitter:image" content="https://ahxt.github.io//xt.png"><meta property="twitter:image:alt" content="Xiaotian Han - Profile Photo"><meta property="twitter:creator" content="@XiaotianHan1"><!-- Additional Meta Tags for Academic Site --><meta name="theme-color" content="#1e40af"><meta name="msapplication-TileColor" content="#1e40af"><!-- Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","name":"Xiaotian Han","headline":"Research Article - Xiaotian Han | Academic Insights","jobTitle":"Assistant Professor of Computer Science","worksFor":{"@type":"Organization","name":"Case Western Reserve University","url":"https://case.edu"},"alumniOf":{"@type":"Organization","name":"Texas A&M University"},"knowsAbout":["Machine Learning","Large Language Models","Computer Science","Artificial Intelligence","LLMs"],"url":"https://ahxt.github.io/blog/2024-12-30-inference-time-scaling-exp/","image":"https://ahxt.github.io//xt.png","sameAs":["https://scholar.google.com/citations?hl=en&user=Uromx98AAAAJ&view_op=list_works&sortby=pubdate","https://x.com/XiaotianHan1","https://bsky.app/profile/xhan2.bsky.social","https://github.com/ahxt"],"author":{"@type":"Person","name":"Xiaotian Han"},"publisher":{"@type":"Person","name":"Xiaotian Han"},"datePublished":"2025-06-28T03:59:49.016Z","dateModified":"2025-06-28T03:59:49.016Z","description":"Academic research article by Xiaotian Han on machine learning and large language models."}</script><!-- Preconnect to external domains for performance --><link rel="preconnect" href="https://scholar.google.com"><link rel="preconnect" href="https://github.com"><link rel="dns-prefetch" href="https://x.com"><link rel="dns-prefetch" href="https://bsky.app"><header> <nav> <div class="logo-container"> <h2 class="logo"> <a href="/" class="logo-link"> <span class="logo-text">Xiaotian Han</span> </a> </h2> </div> <div class="internal-links"> <a href="/" class="nav-link "> <span class="nav-text">About</span> <div class="nav-indicator"></div> </a> <a href="/blog" class="nav-link active"> <span class="nav-text">Blog</span> <div class="nav-indicator"></div> </a> </div> </nav> </header> <div class="footer" hidden="hidden"> <div class="center"> <a><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=m&d=91g_Uih-7fadH9madF_Vex1LQXOVlduL5aeBBSKXgXA&co=2d78ad&ct=ffffff"></a> </div> </div> <main> <div> <article> <div class="post-header"> <h4>Reproduce the inference time scaling exp</h4> <div class="post-meta"> <div class="date"> December 29, 2024 </div> <div class="author">by Xiaotian Han</div> </div> </div> <div class="content"> <div>
  <div>1.Takeaways</div>
  <div>2.Dataset and model</div>
  <div>3.Reproduce results</div>
  <div>4.Performance improvement in terms of flops?</div>
  <div>5.Summary</div>
  <p>In this blog post, I share my reproduction of <a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute">huggingface blogpost-scaling-test-time-compute</a>. The goal is to show that with more generated tokens, the performance of a smaller model can approach that of a larger model.</p>
  <h2>1. Takeaways</h2>
  <div>
    <ul>
      <li><strong>Answer Extraction</strong>: Parsing the final answer out of raw LLM responses is often non-trivial, as different models or prompt formats can wrap the result in extra tokens.</li>
      <li><strong>Special Tokens</strong>: Be mindful of tokens like <code>&lt;|begin_of_text|></code> that may appear in outputs for some models.</li>
      <li><strong>Smaller Models Benefit More</strong>: When we sample multiple solutions, smaller models see a larger relative improvement in accuracy compared to bigger models.</li>
      <li><strong>Bigger Models Still Win</strong>: Even after scaling smaller models heavily at inference, bigger models can still achieve higher absolute accuracy.</li>
      <li><strong>FLOPs Analysis</strong>: Realistically, sampling many candidate solutions quickly becomes computationally expensive. Will scaling the test-time computing improve the performance in terms of flops?</li>
      <li>The code is available at this <a href="https://github.com/ahxt/scaling-test-time-compute-reproduce">github repo</a>.</li>
    </ul>
  </div>
  <h2>2. Dataset and model</h2>
  <h3>2.1. dataset</h3>
  <p>The dataset used in this experiment is <a href="https://huggingface.co/datasets/HuggingFaceH4/MATH-500">HuggingFaceH4/MATH-500</a>. It consists of 500 problems from the MATH benchmark, each containing:</p>
  <pre>problem: Convert the point $(0,3)$ in rectangular coordinates to polar coordinates. Enter your answer in the form $(r,\theta),$ where $r > 0$ and $0 \le \theta &lt; 2 \pi.$<br><br>solution: We have that $r = \sqrt{0^2 + 3^2} = 3.$ Also, if we draw the line connecting the origin and $(0,3),$ this line makes an angle of $\frac{\pi}{2}$ with the positive $x$-axis. [asy] unitsize(0.8 cm); draw((-0.5,0)--(3.5,0)); draw((0,-0.5)--(0,3.5)); draw(arc((0,0),3,0,90),red,Arrow(6)); dot((0,3), red); label("$(0,3)$", (0,3), W); dot((3,0), red); [/asy] Therefore, the polar coordinates are $\boxed{\left( 3, \frac{\pi}{2} \right)}.$<br><br>answer: \left( 3, \frac{\pi}{2} \right)</pre>
  <h3>2.2. Large language models</h3>
  <p>I evaluate two models Llama and Qwen with different sizes:</p>
  <ul>
    <li>
      <p>Llama</p>
      <ul>
        <li><a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct">Llama-3.2-1B-Instruct</a></li>
        <li><a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">Llama-3.1-8B-Instruct</a></li>
      </ul>
    </li>
    <li>
      <p>Qwen</p>
      <ul>
        <li><a href="https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct">Qwen2.5-0.5B-Instruct</a></li>
        <li><a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct">Qwen2.5-1.5B-Instruct</a></li>
        <li><a href="https://huggingface.co/Qwen/Qwen2.5-3B-Instruct">Qwen2.5-3B-Instruct</a></li>
        <li><a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a></li>
        <li><a href="https://huggingface.co/Qwen/Qwen2.5-14B-Instruct">Qwen2.5-14B-Instruct</a></li>
      </ul>
    </li>
  </ul>
  <h3>2.3. Reward model</h3>
  <p><a href="https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data">Llama3.1-8B-PRM-Deepseek-Data</a></p>
  <p>The model is trained from meta-llama/Llama-3.1-8B-Instruct on RLHFlow/Deepseek-PRM-Data for 1 epochs. This model can be used for ORM and PRM. ORM evaluates the final solution, while PRM measures logical correctness at each computation step.</p>
  <ul>
    <li>ORM: extract the probability of <code>"+"</code> from the assistant. It represents the outcome reward score for this answer.</li>
  </ul>
  <pre>[<br>      {"role": "user", "content": "Convert the point $(0,3)$ in rectangular coordinates to polar coordinates. To convert from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\theta)$, we can use the formulas\n\\[r = \\sqrt{x^2 + y^2}\\]\n\\[\\theta = \\arctan \\frac{y}{x}\\]\n\nIn this case, the rectangular coordinates are $(0,3)$, so $x = 0$ and $y = 3$. \n\nFirst, we calculate $r$:\n\\[r = \\sqrt{0^2 + 3^2} = \\sqrt{9} = 3\\]\n\nNext, we calculate $\\theta$:\n\\[\\theta = \\arctan \\frac{3}{0}\\]\nSince the tangent function is not defined for $x = 0$, we need to use a special case. When $x = 0$, $\\theta = \\frac{\\pi}{2}$ if $y > 0$, and $\\theta = \\frac{3\\pi}{2}$ if $y &lt; 0$. In this case, $y = 3 > 0$, so $\\theta = \\frac{\\pi}{2}$.\n\nSo, the polar coordinates equivalent to $(0,3)$ are $\\boxed{(3,\\frac{\\pi}{2})}$."},<br>      {"role": "assistant", "content": "+"},<br>]</pre>
  <ul>
    <li>PRM: computes step-wise reward scores by analyzing each interaction. extract the probability of <code>"+"</code> from the assistant in each turn.</li>
  </ul>
  <pre>[<br>      {"role": "user", "content": "Convert the point $(0,3)$ in rectangular coordinates to polar coordinates. To convert from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\theta)$, we can use the formulas\n\\[r = \\sqrt{x^2 + y^2}\\]\n\\[\\theta = \\arctan \\frac{y}{x}\\]"},<br>      {"role": "assistant", "content": "+"},<br>      {"role": "user", "content": "In this case, the rectangular coordinates are $(0,3)$, so $x = 0$ and $y = 3$."},<br>      {"role": "assistant", "content": "+"},<br>      {"role": "user", "content": "In this case, $y = 3 > 0$, so $\\theta = \\frac{\\pi}{2}$."},<br>      {"role": "assistant", "content": "+"},<br>      {"role": "user", "content": "So, the polar coordinates equivalent to $(0,3)$ are $\\boxed{(3,\\frac{\\pi}{2})}$."},<br>      {"role": "assistant", "content": "+"},<br>]</pre>
  <h3>2.4. Test-time scaling strategies</h3>
  <ul>
    <li>
      <p><strong>majority voting</strong></p>
      <ul>
        <li>generate <span><math><mi>𝑁</mi></math></span> candidate solutions and pick the most frequent answer</li>
      </ul>
    </li>
    <li>
      <p><strong>best of <span><math><mi>𝑁</mi></math></span></strong>:</p>
      <ul>
        <li>(vanilla) generate <span><math><mi>𝑁</mi></math></span> candidates and pick the one with the highest score</li>
        <li>(weighted) generate <span><math><mi>𝑁</mi></math></span> candidates and group the indentical answers, then pick the one with the highest score</li>
      </ul>
    </li>
    <li>
      <p><strong>Beam search</strong>:</p>
      <ul>
        <li>[WIP]</li>
      </ul>
    </li>
  </ul>
  <h2>3. Reproduce results</h2>
  <h3>3.1. obersevations</h3>
  <div>
    <ul>
      <li>for qwen, majority voting and weighted best-of-N achieve similar performance.</li>
      <li>scaling test-time computing benefits smaller models more significantly than larger ones.</li>
      <li>larger models still outperform smaller ones, even with test-time scaling.</li>
    </ul>
  </div>
  <h2>4. Performance improvement in terms of flops?</h2>
  <p>A natural question: Does scaling the test-time compute yield consistent improvements if we measure actual FLOPs cost rather than just the number of generated tokens?</p>
  <p>Different model sizes have different computational demands. Additionally, for inference, the FLOPs for prefill (the forward pass over the prompt) and decoding (token-by-token generation) are quite different. For the PRM approach, there's an extra overhead of the reward model forward pass. For different size of models, the inference flops may not be liner to the model size. thus I want to see if the performance improvement in terms of flops is consistent with the number of generated tokens.</p>
  <ul>
    <li>for majority voting, the total FLOPs is prefill FLOPs + decode FLOPs <span><math><mo>×</mo></math></span> N.</li>
    <li>for weighted best-of-N, the total FLOPs is prefill FLOPs + decode FLOPs <span><math><mo>×</mo></math></span> N + prm FLOPs <span><math><mo>×</mo></math></span> N.</li>
  </ul>
  <p>where <span><math><mi>𝑁</mi></math></span> is the number of samples generated.</p>
  <h3>4.1. LLM FLOPs estimation</h3>
  <p>I estimated the FLOPs of the forward pass for prefill and decoding stages as follows. The equation and the anylysis are based on this paper <a href="https://arxiv.org/pdf/2404.11502">arXiv</a>.</p>
  <p>During the following analysis, I use the following notations:</p>
  <ul>
    <li><span><math><mi>𝑏</mi></math></span> is the batch size</li>
    <li><span><math><mi>𝑠</mi></math></span> is the input sequence length</li>
    <li><span><math><mi>ℎ</mi></math></span> is the hidden size</li>
    <li><span><math><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></math></span> is the FFN intermediate size</li>
    <li><span><math><mi>𝑛</mi></math></span> is the number of heads</li>
    <li><span><math><mi>𝑑</mi></math></span> is the size of each head (<span><math><mrow><mi>ℎ</mi><mo>=</mo><mi>𝑛</mi><mi>𝑑</mi></mrow></math></span>)</li>
  </ul>
  <p>For prefill stage, the equations and corresponding FLOPs are:</p>
  <table>
    <tr>
      <td><math display="block"><mrow><mrow><mi>𝑸</mi><mi>𝑲</mi><mi>𝑽</mi></mrow><mo>=</mo><mrow><mi>𝑿</mi><msub><mi>𝑾</mi><mrow><mi>𝑄</mi><mi>𝐾</mi><mi>𝑉</mi></mrow></msub></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>6</mn><mi>𝑏</mi><mi>𝑠</mi><msup><mi>ℎ</mi><mn>2</mn></msup></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mrow><mi>𝑸</mi><mi>𝑲</mi></mrow><mo>=</mo><mtext> RoPE</mtext><mrow><mo>(</mo><mrow><mi>𝑸</mi><mi>𝑲</mi></mrow><mo>)</mo></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>6</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑶</mi><mo>=</mo><mtext> Attn</mtext><mrow><mo>(</mo><mrow><mi>𝑸</mi><mi>𝑲</mi><mi>𝑽</mi></mrow><mo>)</mo></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>4</mn><mi>𝑏</mi><msup><mi>𝑠</mi><mn>2</mn></msup><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑏</mi><msup><mi>𝑠</mi><mn>2</mn></msup><mi>𝑛</mi></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑿</mi><mo>=</mo><mrow><mi>𝑶</mi><msub><mi>𝑾</mi><mi>𝑂</mi></msub></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>2</mn><mi>𝑏</mi><mi>𝑠</mi><msup><mi>ℎ</mi><mn>2</mn></msup></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑿</mi><mo>=</mo><mtext> Add&amp;Norm</mtext><mrow><mo>(</mo><mi>𝑿</mi><mo>)</mo></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>5</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mrow><mi>𝑮</mi><mi>𝑼</mi></mrow><mo>=</mo><mi>𝑿</mi><mrow><mo>[</mo><mrow><mrow><msub><mi>𝑾</mi><mi>𝐺</mi></msub><mo>,</mo></mrow><msub><mi>𝑾</mi><mi>𝑈</mi></msub></mrow><mo>]</mo></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>4</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑫</mi><mo>=</mo><mtext> Swish</mtext><mrow><mo>(</mo><mi>𝑮</mi><mo>)</mo></mrow><mi>𝑼</mi></mrow></math></td>
      <td><math display="block"><mrow><mn>2</mn><mi>𝑏</mi><mi>𝑠</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑿</mi><mo>=</mo><mi>𝑫</mi><msub><mi>𝑾</mi><mi>𝐷</mi></msub></mrow></math></td>
      <td><math display="block"><mrow><mn>2</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑿</mi><mo>=</mo><mtext> Add&amp;Norm</mtext><mrow><mo>(</mo><mi>𝑿</mi><mo>)</mo></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>5</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi></mrow></math></td>
    </tr>
  </table>
  <p>For decoding stage, the equations and corresponding FLOPs are:</p>
  <table>
    <tr>
      <td><math display="block"><mrow><mrow><mo>(</mo><mrow><mrow><mi>𝑞</mi><mo>,</mo></mrow><mrow><mi>𝑘</mi><mo>,</mo></mrow><mi>𝑣</mi></mrow><mo>)</mo></mrow><mo>=</mo><mrow><mi>𝑥</mi><msub><mi>𝑾</mi><mrow><mi>𝑄</mi><mi>𝐾</mi><mi>𝑉</mi></mrow></msub></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>6</mn><mi>𝑏</mi><msup><mi>ℎ</mi><mn>2</mn></msup></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mrow><mo>(</mo><mrow><mrow><mi>𝑞</mi><mo>,</mo></mrow><mi>𝑘</mi></mrow><mo>)</mo></mrow><mo>=</mo><mtext> RoPE</mtext><mrow><mo>(</mo><mrow><mrow><mi>𝑞</mi><mo>,</mo></mrow><mi>𝑘</mi></mrow><mo>)</mo></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>6</mn><mi>𝑏</mi><mi>ℎ</mi></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mrow><mo>(</mo><mrow><mrow><mi>𝐾</mi><mo>,</mo></mrow><mi>𝑉</mi></mrow><mo>)</mo></mrow><mo>=</mo><mtext> Cache</mtext><mrow><mo>(</mo><mrow><mrow><mi>𝑘</mi><mo>,</mo></mrow><mi>𝑣</mi></mrow><mo>)</mo></mrow></mrow></math></td>
      <td>"-"</td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑜</mi><mo>=</mo><mtext> Attn</mtext><mrow><mo>(</mo><mrow><mrow><mi>𝑞</mi><mo>,</mo></mrow><mrow><mi>𝐾</mi><mo>,</mo></mrow><mi>𝑉</mi></mrow><mo>)</mo></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>4</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑏</mi><mi>𝑠</mi><mi>𝑛</mi></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑥</mi><mo>=</mo><mi>𝑜</mi><msub><mi>𝑾</mi><mi>𝑂</mi></msub></mrow></math></td>
      <td><math display="block"><mrow><mn>2</mn><mi>𝑏</mi><msup><mi>ℎ</mi><mn>2</mn></msup></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑥</mi><mo>=</mo><mtext> Add&amp;Norm</mtext><mrow><mo>(</mo><mi>𝑥</mi><mo>)</mo></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>5</mn><mi>𝑏</mi><mi>ℎ</mi></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mrow><mo>(</mo><mrow><mrow><mi>𝑔</mi><mo>,</mo></mrow><mi>𝑢</mi></mrow><mo>)</mo></mrow><mo>=</mo><mrow><mi>𝑥</mi><mrow><mo>[</mo><mrow><mrow><msub><mi>𝑾</mi><mi>𝐺</mi></msub><mo>,</mo></mrow><msub><mi>𝑾</mi><mi>𝑈</mi></msub></mrow><mo>]</mo></mrow></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>4</mn><mi>𝑏</mi><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑑</mi><mo>=</mo><mtext> Swish</mtext><mrow><mo>(</mo><mi>𝑔</mi><mo>)</mo></mrow><mi>𝑢</mi></mrow></math></td>
      <td><math display="block"><mrow><mn>2</mn><mi>𝑏</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑥</mi><mo>=</mo><mi>𝑑</mi><msub><mi>𝑾</mi><mi>𝐷</mi></msub></mrow></math></td>
      <td><math display="block"><mrow><mn>2</mn><mi>𝑏</mi><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></mrow></math></td>
    </tr>
    <tr>
      <td><math display="block"><mrow><mi>𝑥</mi><mo>=</mo><mtext> Add&amp;Norm</mtext><mrow><mo>(</mo><mi>𝑥</mi><mo>)</mo></mrow></mrow></math></td>
      <td><math display="block"><mrow><mn>5</mn><mi>𝑏</mi><mi>ℎ</mi></mrow></math></td>
    </tr>
  </table>
  <p>For MATH-500 dataset, The FLOPs of the forward pass can be estimated as follows:</p>
  <div>
    <ul>
      <li>prefill FLOPs = <span><math><mrow><mn>6</mn><mi>𝑏</mi><mi>𝑠</mi><msup><mi>ℎ</mi><mn>2</mn></msup><mo>+</mo><mn>6</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi><mo>+</mo><mrow><mo>(</mo><mrow><mn>4</mn><mi>𝑏</mi><msup><mi>𝑠</mi><mn>2</mn></msup><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑏</mi><msup><mi>𝑠</mi><mn>2</mn></msup><mi>𝑛</mi></mrow><mo>)</mo></mrow><mo>+</mo><mn>2</mn><mi>𝑏</mi><mi>𝑠</mi><msup><mi>ℎ</mi><mn>2</mn></msup><mo>+</mo><mn>5</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup><mo>+</mo><mn>2</mn><mi>𝑏</mi><mi>𝑠</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup><mo>+</mo><mn>2</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup><mo>+</mo><mn>5</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi></mrow></math></span></li>
      <li>decoding FLOPs = <span><math><mrow><mn>6</mn><mi>𝑏</mi><msup><mi>ℎ</mi><mn>2</mn></msup><mo>+</mo><mn>6</mn><mi>𝑏</mi><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑏</mi><mi>𝑠</mi><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑏</mi><mi>𝑠</mi><mi>𝑛</mi><mo>+</mo><mn>2</mn><mi>𝑏</mi><msup><mi>ℎ</mi><mn>2</mn></msup><mo>+</mo><mn>5</mn><mi>𝑏</mi><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑏</mi><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup><mo>+</mo><mn>2</mn><mi>𝑏</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup><mo>+</mo><mn>2</mn><mi>𝑏</mi><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup><mo>+</mo><mn>5</mn><mi>𝑏</mi><mi>ℎ</mi></mrow></math></span></li>
    </ul>
  </div>
  <p>I compute the FLOPs of the forward pass for batch size is <span><math><mn>1</mn></math></span>. Then</p>
  <div>
    <ul>
      <li>prefill FLOPs = <span><math><mrow><mn>8</mn><mi>𝑠</mi><msup><mi>ℎ</mi><mn>2</mn></msup><mo>+</mo><mn>16</mn><mi>𝑠</mi><mi>ℎ</mi><mo>+</mo><mn>4</mn><msup><mi>𝑠</mi><mn>2</mn></msup><mi>ℎ</mi><mo>+</mo><mn>4</mn><msup><mi>𝑠</mi><mn>2</mn></msup><mi>𝑛</mi><mo>+</mo><mn>6</mn><mi>𝑠</mi><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup><mo>+</mo><mn>2</mn><mi>𝑠</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></mrow></math></span></li>
      <li>decoding FLOPs = <span><math><mrow><mn>8</mn><msup><mi>ℎ</mi><mn>2</mn></msup><mo>+</mo><mn>16</mn><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑠</mi><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑠</mi><mi>𝑛</mi><mo>+</mo><mn>6</mn><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup><mo>+</mo><mn>2</mn><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></mrow></math></span></li>
    </ul>
  </div>
  <p>Thus I use the following formula to compute the total FLOPs:</p>
  <math display="block"><mrow><msub><mtext>FLOPs </mtext><mrow><mtext>prefill</mtext><mrow><mo>(</mo><mi>𝑠</mi><mo>)</mo></mrow></mrow></msub><mo>=</mo><mn>8</mn><mi>𝑠</mi><msup><mi>ℎ</mi><mn>2</mn></msup><mo>+</mo><mn>16</mn><mi>𝑠</mi><mi>ℎ</mi><mo>+</mo><mn>4</mn><msup><mi>𝑠</mi><mn>2</mn></msup><mi>ℎ</mi><mo>+</mo><mn>4</mn><msup><mi>𝑠</mi><mn>2</mn></msup><mi>𝑛</mi><mo>+</mo><mn>6</mn><mi>𝑠</mi><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup><mo>+</mo><mn>2</mn><mi>𝑠</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></mrow></math><math display="block"><mrow><msub><mtext>FLOPs </mtext><mrow><mtext>decode</mtext><mrow><mo>(</mo><mi>𝑠</mi><mo>)</mo></mrow></mrow></msub><mo>=</mo><mn>8</mn><msup><mi>ℎ</mi><mn>2</mn></msup><mo>+</mo><mn>16</mn><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑠</mi><mi>ℎ</mi><mo>+</mo><mn>4</mn><mi>𝑠</mi><mi>𝑛</mi><mo>+</mo><mn>6</mn><mi>ℎ</mi><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup><mo>+</mo><mn>2</mn><msup><mi>ℎ</mi><mo lspace="0em" rspace="0em" style="padding-left: 0.08em">′</mo></msup></mrow></math><math display="block"><mrow><msub><mtext>FLOPs </mtext><mtext>total </mtext></msub><mo>=</mo><msub><mtext> FLOPs </mtext><mrow><mtext>prefill</mtext><mrow><mo>(</mo><msub><mi>𝑝</mi><mi>𝑙</mi></msub><mo>)</mo></mrow></mrow></msub><mo>+</mo><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑖</mi><mo>=</mo><mn>0</mn></mrow><mrow><msub><mi>𝑑</mi><mi>𝑙</mi></msub><mo>−</mo><mn>1</mn></mrow></munderover><msub><mtext> FLOPs</mtext><mrow><mtext>decode</mtext><mrow><mo>(</mo><mrow><msub><mi>𝑝</mi><mi>𝑙</mi></msub><mo>+</mo><mi>𝑖</mi></mrow><mo>)</mo></mrow></mrow></msub></mrow></math>
  <p>where <span><math><msub><mi>𝑝</mi><mi>𝑙</mi></msub></math></span> is the length of the problem prompt, and <span><math><msub><mi>𝑑</mi><mi>𝑙</mi></msub></math></span> is the number of tokens we generate for the solution.</p>
  <h3>4.2. results</h3>
  <p>Below, we re-plot the same data—accuracy vs. total FLOPs—for Qwen2.5 of various sizes. The left endpoint of each curve (for majority voting) corresponds to the minimal compute cost of a greedy decoding (<span><math><mrow><mi>𝑁</mi><mo>=</mo><mn>1</mn></mrow></math></span>). As the inference time move right, (ideally) smaller models with less flops can achieve similar performance to larger models with more flops.</p>
  <p>The results are shown below:</p>
  <h3>4.3. obersevations</h3>
  <div>
    <ul>
      <li>Majority Voting seems to achieve a slightly better cost-to-performance trade-off than Weighted Best-of-N (in some cases). The overhead of scoring each candidate can become significant if <span><math><mi>𝑁</mi></math></span> is large.</li>
      <li>Scaling for smaller models remains beneficial, but diminishing returns do appear at higher <span><math><mi>𝑁</mi></math></span>. If you keep increasing <span><math><mi>𝑁</mi></math></span>, you might pay a lot more FLOPs for only marginal accuracy gains.</li>
      <li>Larger model vs. scaled smaller model: Even if a smaller model is heavily scaled in test-time compute, a properly sized larger model may still achieve a strictly higher accuracy while also being less or similarly expensive in total FLOPs.</li>
    </ul>
  </div>
  <h2>5. Summary</h2>
  <p>This reproduction reaffirms the main conclusion from the Hugging Face blog post: scaling test-time compute (by sampling multiple solutions and picking the best or majority) can improve accuracy, especially for smaller models. Yet, these improvements don't entirely overcome the fundamental quality gap between smaller and larger models.</p>
  <p>We further demonstrate how analyzing FLOPs clarifies the computational trade-offs in test-time scaling. It's not always free to sample or evaluate more solutions. Practitioners need to weigh the cost-to-benefit ratio carefully, particularly if they aim to deploy these methods at scale.</p>
</div>
 </div> <footer> <a href="/blog" class="back-link">← Back to Blog</a> </footer> </article> </div> </main> </body></html>