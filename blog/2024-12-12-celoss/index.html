<!DOCTYPE html><html lang="en"> <head><meta charset="utf-8"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><meta name="viewport" content="width=device-width"><meta name="generator" content="Astro v5.10.1"><title>Cross-entropy loss and its optimization [WIP] - Academic Homepage</title><link rel="stylesheet" href="https://fred-wang.github.io/MathFonts/NewComputerModern/mathfonts.css"><link rel="stylesheet" href="/_astro/blog.CJoC8E46.css"></head> <body class="blog-post"> <!-- SEO Meta Tags --><title>Research Article - Xiaotian Han | Academic Insights</title><meta name="title" content="Research Article - Xiaotian Han | Academic Insights"><meta name="description" content="Academic research article by Xiaotian Han on machine learning and large language models."><meta name="keywords" content="research article, machine learning, large language models, academic research, LLM"><meta name="author" content="Xiaotian Han"><meta name="robots" content="index, follow"><meta name="language" content="en"><meta name="revisit-after" content="7 days"><!-- Canonical URL --><link rel="canonical" href="https://ahxt.github.io/blog/2024-12-12-celoss/"><!-- Open Graph / Facebook --><meta property="og:type" content="article"><meta property="og:url" content="https://ahxt.github.io/blog/2024-12-12-celoss/"><meta property="og:title" content="Research Article - Xiaotian Han | Academic Insights"><meta property="og:description" content="Academic research article by Xiaotian Han on machine learning and large language models."><meta property="og:image" content="https://ahxt.github.io//xt.png"><meta property="og:image:alt" content="Xiaotian Han - Profile Photo"><meta property="og:site_name" content="Xiaotian Han - Academic Homepage"><meta property="og:locale" content="en_US"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://ahxt.github.io/blog/2024-12-12-celoss/"><meta property="twitter:title" content="Research Article - Xiaotian Han | Academic Insights"><meta property="twitter:description" content="Academic research article by Xiaotian Han on machine learning and large language models."><meta property="twitter:image" content="https://ahxt.github.io//xt.png"><meta property="twitter:image:alt" content="Xiaotian Han - Profile Photo"><meta property="twitter:creator" content="@XiaotianHan1"><!-- Additional Meta Tags for Academic Site --><meta name="theme-color" content="#1e40af"><meta name="msapplication-TileColor" content="#1e40af"><!-- Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","name":"Xiaotian Han","headline":"Research Article - Xiaotian Han | Academic Insights","jobTitle":"Assistant Professor of Computer Science","worksFor":{"@type":"Organization","name":"Case Western Reserve University","url":"https://case.edu"},"alumniOf":{"@type":"Organization","name":"Texas A&M University"},"knowsAbout":["Machine Learning","Large Language Models","Computer Science","Artificial Intelligence","LLMs"],"url":"https://ahxt.github.io/blog/2024-12-12-celoss/","image":"https://ahxt.github.io//xt.png","sameAs":["https://scholar.google.com/citations?hl=en&user=Uromx98AAAAJ&view_op=list_works&sortby=pubdate","https://x.com/XiaotianHan1","https://bsky.app/profile/xhan2.bsky.social","https://github.com/ahxt"],"author":{"@type":"Person","name":"Xiaotian Han"},"publisher":{"@type":"Person","name":"Xiaotian Han"},"datePublished":"2025-06-28T03:59:49.028Z","dateModified":"2025-06-28T03:59:49.028Z","description":"Academic research article by Xiaotian Han on machine learning and large language models."}</script><!-- Preconnect to external domains for performance --><link rel="preconnect" href="https://scholar.google.com"><link rel="preconnect" href="https://github.com"><link rel="dns-prefetch" href="https://x.com"><link rel="dns-prefetch" href="https://bsky.app"><header> <nav> <div class="logo-container"> <h2 class="logo"> <a href="/" class="logo-link"> <span class="logo-text">Xiaotian Han</span> </a> </h2> </div> <div class="internal-links"> <a href="/" class="nav-link "> <span class="nav-text">About</span> <div class="nav-indicator"></div> </a> <a href="/blog" class="nav-link active"> <span class="nav-text">Blog</span> <div class="nav-indicator"></div> </a> </div> </nav> </header> <div class="footer" hidden="hidden"> <div class="center"> <a><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=m&d=91g_Uih-7fadH9madF_Vex1LQXOVlduL5aeBBSKXgXA&co=2d78ad&ct=ffffff"></a> </div> </div> <main> <div> <article> <div class="post-header"> <h4>Cross-entropy loss and its optimization [WIP]</h4> <div class="post-meta"> <div class="date"> December 11, 2024 </div> <div class="author">by Xiaotian Han</div> </div> </div> <div class="content"> <div>
  <div>1.Background</div>
  <div>2.Softmax Cross-Entropy</div>
  <div>3.Linear-Softmax-Cross-Entropy</div>
  <div>4.Optimization Strategies</div>
  <h2>1. Background</h2>
  <p>Computing cross-entropy loss becomes significantly more challenging for LLMs. This is primarily due to the extremely large logit and label matrices involved in the calculations, which can lead to high computational costs and memory usage. Recently, several optimization strategies have been proposed to address this issue, starting from a Pytorch GitHub issue.</p>
  <ul>
    <li><a href="https://github.com/pytorch/pytorch/issues/124480">https://github.com/pytorch/pytorch/issues/124480</a></li>
    <li><a href="https://github.com/mgmalek/efficient_cross_entropy">https://github.com/mgmalek/efficient_cross_entropy</a></li>
    <li>Liger Kernel: <a href="https://github.com/linkedin/Liger-Kernel">github</a>, <a href="https://arxiv.org/pdf/2410.10989">arxiv</a></li>
    <li>Cut Your Losses in Large-Vocabulary Language Models: <a href="https://arxiv.org/pdf/2411.09009">arxiv</a></li>
  </ul>
  <p>All these approaches share a common goal: avoiding the full materialization of the logit matrix. They achieve this by:</p>
  <ol>
    <li value="1">chunking the logit matrix</li>
    <li value="2">computating the gradient of logit in place</li>
  </ol>
  <p>In this blog, I will dive into the cross entropy loss and its optimization strategies.</p>
  <h2>2. Softmax Cross-Entropy</h2>
  <h3>2.1. Forward Pass</h3>
  <p>Let's begin by understanding the <strong>forward pass</strong> of the cross-entropy loss.</p>
  <p>Consider:</p>
  <ul>
    <li>An input vector <span><math><mrow><mi>𝒙</mi><mo>∈</mo><msup><mi mathvariant="normal">ℝ</mi><mi>𝑑</mi></msup></mrow></math></span> representing the logits (unnormalized scores) produced by the model for each class.</li>
    <li>A true label <span><math><mrow><mi>𝑦</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mrow><mn>0</mn><mo>,</mo></mrow><mrow><mn>1</mn><mo>,</mo></mrow><mrow><mo>…</mo><mo>,</mo></mrow><mrow><mi>𝑑</mi><mo>−</mo><mn>1</mn></mrow></mrow><mo>}</mo></mrow></mrow></math></span> indicating the correct class.</li>
  </ul>
  <p>The <strong>softmax function</strong> converts the logits into probabilities:</p>
  <math display="block"><mrow><msub><mi>𝒑</mi><mi>𝑖</mi></msub><mo>=</mo><mfrac><msup><mi>𝑒</mi><msub><mi>𝒙</mi><mi>𝑖</mi></msub></msup><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑑</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒙</mi><mi>𝑘</mi></msub></msup></mrow></mfrac></mrow></math>
  <p>Here, <span><math><msub><mi>𝒑</mi><mi>𝑖</mi></msub></math></span> represents the probability of the input belonging to class <span><math><mi>𝑖</mi></math></span>.</p>
  <p>The <strong>cross-entropy loss</strong> for a single instance is then defined as:</p>
  <math display="block"><mrow><mi>𝐿</mi><mo>=</mo><mrow><mo>−</mo><mrow><mi mathvariant="normal">log</mi><mrow><mo>(</mo><msub><mi>𝒑</mi><mi>𝑦</mi></msub><mo>)</mo></mrow></mrow></mrow></mrow></math>
  <p>Expanding this, we get:</p>
  <math display="block"><mrow><mi>𝐿</mi><mo>=</mo><mrow><mo>−</mo><mrow><mi mathvariant="normal">log</mi><mrow><mo>(</mo><msub><mi>𝒑</mi><mi>𝑦</mi></msub><mo>)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><mi mathvariant="normal">log</mi><mrow><mo>(</mo><mfrac><msup><mi>𝑒</mi><msub><mi>𝒙</mi><mi>𝑦</mi></msub></msup><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑑</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒙</mi><mi>𝑘</mi></msub></msup></mrow></mfrac><mo>)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mo>−</mo><mrow><mi mathvariant="normal">log</mi><mrow><mo>(</mo><msup><mi>𝑒</mi><msub><mi>𝒙</mi><mi>𝑦</mi></msub></msup><mo>)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi mathvariant="normal">log</mi><mrow><mo>(</mo><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑑</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒙</mi><mi>𝑘</mi></msub></msup></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo>−</mo><msub><mi>𝒙</mi><mi>𝑦</mi></msub></mrow><mo>+</mo><mrow><mi mathvariant="normal">log</mi><mrow><mo>(</mo><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑑</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒙</mi><mi>𝑘</mi></msub></msup></mrow><mo>)</mo></mrow></mrow></mrow></math>
  <h3>2.2. Backward Pass</h3>
  <p>In general, the gradient of the loss with respect to the input is given by</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow></mfrac><mfrac><mrow><mo>∂</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac></mrow></math>
  <h4>2.2.1. Step 1: Compute <span><math><mfrac><mrow><mo>∂</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac></math></span></h4>
  <p>The result is:</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac><mo>=</mo><mrow><mo>{</mo><mtable><mtr><mtd class="mathyml-align-left"><msub><mi>𝒑</mi><mrow><mi>𝑗</mi><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mo>)</mo></mrow></mrow></msub></mtd></mtr><mtr><mtd class="mathyml-align-left"><mrow><mtext>if  </mtext><mi>𝑗</mi><mo>=</mo><mi>𝑖</mi></mrow></mtd></mtr><mtr><mtd class="mathyml-align-left"><mrow><mrow><mo>−</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><msub><mi>𝒑</mi><mi>𝑖</mi></msub></mrow></mtd></mtr><mtr><mtd class="mathyml-align-left"><mrow><mtext>if  </mtext><mi>𝑗</mi><mo>≠</mo><mi>𝑖</mi></mrow></mtd></mtr></mtable></mrow></mrow></math>
  <p>The full derivation for the case <span><math><mrow><mi>𝑗</mi><mo>=</mo><mi>𝑖</mi></mrow></math></span> is:</p>
  <math display="block"><mtable><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑗</mi></msub></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mo>∂</mo><mrow><mo>(</mo><mfrac><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑗</mi></msub></msup><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑘</mi></msub></msup></mrow></mfrac><mo>)</mo></mrow></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑗</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mrow><mrow><mo>(</mo><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑘</mi></msub></msup></mrow><mo>)</mo></mrow><mo>⋅</mo><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑗</mi></msub></msup><mo>−</mo><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑗</mi></msub></msup><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑗</mi></msub></msup></mrow></mrow><mrow><msup><mrow><mo>(</mo><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑘</mi></msub></msup></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mrow><mo>(</mo><mfrac><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑗</mi></msub></msup><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑘</mi></msub></msup></mrow></mfrac><mo>)</mo></mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><mfrac><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑗</mi></msub></msup><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑘</mi></msub></msup></mrow></mfrac></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mo>)</mo></mrow></mtd></mtr></mtable></math>
  <p>And for <span><math><mrow><mi>𝑗</mi><mo>≠</mo><mi>𝑖</mi></mrow></math></span>:</p>
  <math display="block"><mtable><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mo>∂</mo><mrow><mo>(</mo><mfrac><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑗</mi></msub></msup><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑘</mi></msub></msup></mrow></mfrac><mo>)</mo></mrow></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mfrac><mrow><mrow><mrow><mo>−</mo><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑗</mi></msub></msup></mrow><mo>⋅</mo><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑖</mi></msub></msup></mrow></mrow><mrow><msup><mrow><mo>(</mo><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msup><mi>𝑒</mi><msub><mi>𝒛</mi><mi>𝑘</mi></msub></msup></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mo>−</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub><msub><mi>𝒑</mi><mi>𝑖</mi></msub></mtd></mtr></mtable></math>
  <h4>2.2.2. Step 2: Compute <span><math><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac></math></span></h4>
  <math display="block"><mtable><mtr><mtd class="mathyml-align-right"><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac></mtd><mtd class="mathyml-align-left"><mo>=</mo><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑗</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><mfrac><mrow><mo>∂</mo><mrow><mo>(</mo><mrow><mrow><mo>−</mo><msub><mi>𝒕</mi><mi>𝑗</mi></msub></mrow><mi mathvariant="normal">log</mi><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mo>−</mo><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑗</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msub><mi>𝒕</mi><mi>𝑗</mi></msub><mfrac><mrow><mo>∂</mo><mrow><mo>(</mo><mrow><mi mathvariant="normal">log</mi><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mo>−</mo><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑗</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msub><mi>𝒕</mi><mi>𝑗</mi></msub><mfrac><mn>1</mn><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mfrac><mfrac><mrow><mo>∂</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mo>−</mo><mfrac><msub><mi>𝒕</mi><mi>𝑖</mi></msub><msub><mi>𝒑</mi><mi>𝑖</mi></msub></mfrac><mfrac><mrow><mo>∂</mo><msub><mi>𝒑</mi><mi>𝑖</mi></msub></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac><mo>−</mo><munderover displaystyle="true"><mo>∑</mo><mrow><mrow><mi>𝑗</mi><mo>=</mo><mn>1</mn><mo>,</mo></mrow><mrow><mi>𝑗</mi><mo>≠</mo><mi>𝑖</mi></mrow></mrow><mi>𝑁</mi></munderover><mfrac><msub><mi>𝒕</mi><mi>𝑗</mi></msub><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mfrac><mfrac><mrow><mo>∂</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><mrow><mo>∂</mo><msub><mi>𝒛</mi><mi>𝑖</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mo>−</mo><mfrac><msub><mi>𝒕</mi><mi>𝑖</mi></msub><msub><mi>𝒑</mi><mi>𝑖</mi></msub></mfrac><msub><mi>𝒑</mi><mrow><mi>𝑖</mi><mrow><mo>(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>𝒑</mi><mi>𝑖</mi></msub></mrow><mo>)</mo></mrow></mrow></msub><mo>−</mo><munderover displaystyle="true"><mo>∑</mo><mrow><mrow><mi>𝑗</mi><mo>=</mo><mn>1</mn><mo>,</mo></mrow><mrow><mi>𝑗</mi><mo>≠</mo><mi>𝑖</mi></mrow></mrow><mi>𝑁</mi></munderover><mfrac><msub><mi>𝒕</mi><mi>𝑗</mi></msub><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mfrac><mrow><mo>(</mo><mrow><mrow><mo>−</mo><msub><mi>𝒑</mi><mi>𝑗</mi></msub></mrow><msub><mi>𝒑</mi><mi>𝑖</mi></msub></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mo>−</mo><msub><mi>𝒕</mi><mi>𝑖</mi></msub><mo>+</mo><msub><mi>𝒕</mi><mi>𝑖</mi></msub><msub><mi>𝒑</mi><mi>𝑖</mi></msub><mo>+</mo><munderover displaystyle="true"><mo>∑</mo><mrow><mrow><mi>𝑗</mi><mo>=</mo><mn>1</mn><mo>,</mo></mrow><mrow><mi>𝑗</mi><mo>≠</mo><mi>𝑖</mi></mrow></mrow><mi>𝑁</mi></munderover><msub><mi>𝒕</mi><mi>𝑗</mi></msub><msub><mi>𝒑</mi><mi>𝑖</mi></msub></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mo>−</mo><msub><mi>𝒕</mi><mi>𝑖</mi></msub><mo>+</mo><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑗</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msub><mi>𝒕</mi><mi>𝑗</mi></msub><msub><mi>𝒑</mi><mi>𝑖</mi></msub></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mo>−</mo><msub><mi>𝒕</mi><mi>𝑖</mi></msub><mo>+</mo><msub><mi>𝒑</mi><mi>𝑖</mi></msub><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑗</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑁</mi></munderover><msub><mi>𝒕</mi><mi>𝑗</mi></msub></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><mo>−</mo><msub><mi>𝒕</mi><mi>𝑖</mi></msub><mo>+</mo><msub><mi>𝒑</mi><mi>𝑖</mi></msub></mtd></mtr><mtr><mtd class="mathyml-align-right"></mtd><mtd class="mathyml-align-left"><mo>=</mo><msub><mi>𝒑</mi><mi>𝑖</mi></msub><mo>−</mo><msub><mi>𝒕</mi><mi>𝑖</mi></msub></mtd></mtr></mtable></math>
  <p>So,</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><mi>𝒛</mi></mrow></mfrac><mo>=</mo><mi>𝒑</mi><mo>−</mo><mi>𝒕</mi></mrow></math>
  <h3>2.3. Gradient in Matrix Form</h3>
  <p>For batch computations, it's efficient to represent gradients in matrix form.</p>
  <p>Given:</p>
  <ul>
    <li><span><math><mrow><mi>𝑷</mi><mo>∈</mo><msup><mi mathvariant="normal">ℝ</mi><mrow><mi>𝑛</mi><mo>×</mo><mi>𝑑</mi></mrow></msup></mrow></math></span>: Matrix of predicted probabilities for a batch of size <span><math><mi>𝑛</mi></math></span>.</li>
    <li><span><math><mrow><mi>𝒁</mi><mo>∈</mo><msup><mi mathvariant="normal">ℝ</mi><mrow><mi>𝑛</mi><mo>×</mo><mi>𝑑</mi></mrow></msup></mrow></math></span>: Matrix of logits.</li>
    <li><span><math><mrow><mi>𝒀</mi><mo>∈</mo><msup><mi mathvariant="normal">ℝ</mi><mrow><mi>𝑛</mi><mo>×</mo><mi>𝑑</mi></mrow></msup></mrow></math></span>: One-hot encoded true labels.</li>
  </ul>
  <p>The gradient with respect to the logits is:</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><msub><mi>𝑷</mi><mrow><mrow><mi>𝑖</mi><mo>,</mo></mrow><mi>𝑗</mi></mrow></msub></mrow><mrow><mo>∂</mo><msub><mi>𝒁</mi><mrow><mrow><mi>𝑖</mi><mo>,</mo></mrow><mi>𝑘</mi></mrow></msub></mrow></mfrac><mo>=</mo><msub><mi>𝑷</mi><mrow><mrow><mi>𝑖</mi><mo>,</mo></mrow><mi>𝑗</mi></mrow></msub><mrow><mo>(</mo><mrow><msub><mi>𝛿</mi><mrow><mrow><mi>𝑗</mi><mo>,</mo></mrow><mi>𝑘</mi></mrow></msub><mo>−</mo><msub><mi>𝑷</mi><mrow><mrow><mi>𝑖</mi><mo>,</mo></mrow><mi>𝑘</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></math><math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><mi>𝒁</mi></mrow></mfrac><mo>=</mo><mi>𝑷</mi><mo>−</mo><mi>𝒀</mi></mrow></math>
  <p>Normalized by batch size, the overall gradient of the loss is:</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><mi>𝒁</mi></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mi>𝑛</mi></mfrac><mrow><mo>(</mo><mrow><mi>𝑷</mi><mo>−</mo><mi>𝒀</mi></mrow><mo>)</mo></mrow></mrow></math>
  <h2>3. Linear-Softmax-Cross-Entropy</h2>
  <p>Cross-entropy loss is typically preceded by a <strong>linear (fully connected) layer</strong> and followed by a <strong>softmax activation</strong>. If we can fuse the linear layer and softmax activation, we may avoid the full materialization of the logit matrix.</p>
  <ul>
    <li>Input before the final linear layer: <span><math><mrow><mi>𝑿</mi><mo>∈</mo><msup><mi mathvariant="normal">ℝ</mi><mrow><mi>𝑛</mi><mo>×</mo><msub><mi>𝑑</mi><mtext> in</mtext></msub></mrow></msup></mrow></math></span></li>
    <li>Linear weights: <span><math><mrow><mi>𝑾</mi><mo>∈</mo><msup><mi mathvariant="normal">ℝ</mi><mrow><msub><mi>𝑑</mi><mtext>in </mtext></msub><mo>×</mo><msub><mi>𝑑</mi><mtext> out</mtext></msub></mrow></msup></mrow></math></span></li>
    <li>Linear bias: <span><math><mrow><mi>𝒃</mi><mo>∈</mo><msup><mi mathvariant="normal">ℝ</mi><msub><mi>𝑑</mi><mtext> out</mtext></msub></msup></mrow></math></span></li>
    <li>Labels: <span><math><mrow><mi>𝑦</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mrow><mn>0</mn><mo>,</mo></mrow><mrow><mn>1</mn><mo>,</mo></mrow><mrow><mo>…</mo><mo>,</mo></mrow><mrow><mi>𝑛</mi><mo>−</mo><mn>1</mn></mrow></mrow><mo>}</mo></mrow></mrow></math></span>, representing the true classes for each instance in the batch.</li>
  </ul>
  <h3>3.1. Forward Pass</h3>
  <p>With a linear transformation, the input <span><math><mi>𝑿</mi></math></span> is transformed linearly using the weights and bias:</p>
  <math display="block"><mrow><mi>𝒁</mi><mo>=</mo><mi>𝑿</mi><mi>𝑾</mi><mo>+</mo><mi>𝒃</mi></mrow></math>
  <p>Softmax:</p>
  <math display="block"><mrow><msub><mi>𝑷</mi><mrow><mrow><mi>𝑖</mi><mo>,</mo></mrow><mi>𝑗</mi></mrow></msub><mo>=</mo><mfrac><msup><mi>𝑒</mi><msub><mi>𝒁</mi><mrow><mrow><mi>𝑖</mi><mo>,</mo></mrow><mi>𝑗</mi></mrow></msub></msup><mrow><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑘</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>𝑑</mi><mtext>out </mtext></msub></munderover><msup><mi>𝑒</mi><msub><mi>𝒁</mi><mrow><mrow><mi>𝑖</mi><mo>,</mo></mrow><mi>𝑘</mi></mrow></msub></msup></mrow></mfrac></mrow></math>
  <p>Cross-entropy loss is computed for each instance and then averaged over the batch:</p>
  <math display="block"><mrow><msub><mi>𝐿</mi><mi>𝑖</mi></msub><mo>=</mo><mrow><mo>−</mo><mrow><mi mathvariant="normal">log</mi><mrow><mo>(</mo><msub><mi>𝑷</mi><mrow><mrow><mi>𝑖</mi><mo>,</mo></mrow><msub><mi>𝑦</mi><mi>𝑖</mi></msub></mrow></msub><mo>)</mo></mrow></mrow></mrow></mrow></math><math display="block"><mrow><mi>𝐿</mi><mo>=</mo><mfrac><mn>1</mn><mi>𝑛</mi></mfrac><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑖</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑛</mi></munderover><msub><mi>𝐿</mi><mi>𝑖</mi></msub></mrow></math>
  <h3>3.2. Backward Pass</h3>
  <p>Gradient of <span><math><mi>𝒁</mi></math></span>:</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><mi>𝒁</mi></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mi>𝑛</mi></mfrac><mrow><mo>(</mo><mrow><mi>𝑷</mi><mo>−</mo><mi>𝒀</mi></mrow><mo>)</mo></mrow></mrow></math>
  <p>Gradient of <span><math><mi>𝑾</mi></math></span>:</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><mi>𝑾</mi></mrow></mfrac><mo>=</mo><msup><mi>𝑿</mi><mi>𝑇</mi></msup><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><mi>𝒁</mi></mrow></mfrac></mrow></math>
  <p>Gradient of <span><math><mi>𝒃</mi></math></span>:</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><mi>𝒃</mi></mrow></mfrac><mo>=</mo><munderover displaystyle="true"><mo>∑</mo><mrow><mi>𝑖</mi><mo>=</mo><mn>1</mn></mrow><mi>𝑛</mi></munderover><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><msub><mi>𝒁</mi><mi>𝑖</mi></msub></mrow></mfrac></mrow></math>
  <p>Gradient of input <span><math><mi>𝑿</mi></math></span>:</p>
  <math display="block"><mrow><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><mi>𝑿</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>𝐿</mi></mrow><mrow><mo>∂</mo><mi>𝒁</mi></mrow></mfrac><msup><mi>𝑾</mi><mi>𝑇</mi></msup></mrow></math>
  <h3>3.3. Summary of Gradients</h3>
  <table>
    <tr>
      <td><strong>Parameter</strong></td>
      <td><strong>Formula</strong></td>
      <td><strong>Dimensions</strong></td>
    </tr>
    <tr>
      <td><span><math><mi>𝒁</mi></math></span></td>
      <td><span><math><mrow><mi>𝒁</mi><mo>=</mo><mi>𝑿</mi><mi>𝑾</mi><mo>+</mo><mi>𝒃</mi></mrow></math></span></td>
      <td><span><math><mrow><mo>[</mo><mrow><mrow><mi>𝑛</mi><mo>,</mo></mrow><msub><mi>𝑑</mi><mtext> out</mtext></msub></mrow><mo>]</mo></mrow></math></span></td>
    </tr>
    <tr>
      <td><span><math><mi>𝑷</mi></math></span></td>
      <td><span><math><mrow><mi>𝑷</mi><mo>=</mo><mrow><mtext>softmax</mtext><mrow><mo>(</mo><mi>𝒁</mi><mo>)</mo></mrow></mrow></mrow></math></span></td>
      <td><span><math><mrow><mo>[</mo><mrow><mrow><mi>𝑛</mi><mo>,</mo></mrow><msub><mi>𝑑</mi><mtext> out</mtext></msub></mrow><mo>]</mo></mrow></math></span></td>
    </tr>
    <tr>
      <td><span><math><mi>𝐿</mi></math></span></td>
      <td><span><math><mrow><mi>𝐿</mi><mo>=</mo><mrow><mo>−</mo><mfrac><mn>1</mn><mi>𝑛</mi></mfrac></mrow><mo>∑</mo><mrow><mi mathvariant="normal">log</mi><mrow><mo>(</mo><msub><mi>𝑷</mi><mrow><mrow><mi>𝑖</mi><mo>,</mo></mrow><msub><mi>𝑦</mi><mi>𝑖</mi></msub></mrow></msub><mo>)</mo></mrow></mrow></mrow></math></span></td>
      <td><span><math><mtext>Scalar</mtext></math></span></td>
    </tr>
    <tr>
      <td><span><math><mrow><mi>𝑑</mi><mi>𝒁</mi></mrow></math></span></td>
      <td><span><math><mrow><mi>𝑑</mi><mi>𝒁</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>𝑛</mi><mrow><mo>(</mo><mrow><mi>𝑷</mi><mo>−</mo><mi>𝒀</mi></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></math></span></td>
      <td><span><math><mrow><mo>[</mo><mrow><mrow><mi>𝑛</mi><mo>,</mo></mrow><msub><mi>𝑑</mi><mtext> out</mtext></msub></mrow><mo>]</mo></mrow></math></span></td>
    </tr>
    <tr>
      <td><span><math><mrow><mi>𝑑</mi><mi>𝑾</mi></mrow></math></span></td>
      <td><span><math><mrow><mi>𝑑</mi><mi>𝑾</mi><mo>=</mo><msup><mi>𝑿</mi><mi>𝑇</mi></msup><mi>𝑑</mi><mi>𝒁</mi></mrow></math></span></td>
      <td><span><math><mrow><mo>[</mo><mrow><mrow><msub><mi>𝑑</mi><mtext>in</mtext></msub><mo>,</mo></mrow><msub><mi>𝑑</mi><mtext> out</mtext></msub></mrow><mo>]</mo></mrow></math></span></td>
    </tr>
    <tr>
      <td><span><math><mrow><mi>𝑑</mi><mi>𝒃</mi></mrow></math></span></td>
      <td><span><math><mrow><mi>𝑑</mi><mi>𝒃</mi><mo>=</mo><mrow><mtext>sum</mtext><mrow><mo>(</mo><mrow><mi>𝑑</mi><mi>𝒁</mi></mrow><mo>)</mo></mrow></mrow></mrow></math></span></td>
      <td><span><math><mrow><mo>[</mo><msub><mi>𝑑</mi><mtext>out</mtext></msub><mo>]</mo></mrow></math></span></td>
    </tr>
    <tr>
      <td><span><math><mrow><mi>𝑑</mi><mi>𝑿</mi></mrow></math></span></td>
      <td><span><math><mrow><mi>𝑑</mi><mi>𝑿</mi><mo>=</mo><mi>𝑑</mi><mi>𝒁</mi><msup><mi>𝑾</mi><mi>𝑇</mi></msup></mrow></math></span></td>
      <td><span><math><mrow><mo>[</mo><mrow><mrow><mi>𝑛</mi><mo>,</mo></mrow><msub><mi>𝑑</mi><mtext> in</mtext></msub></mrow><mo>]</mo></mrow></math></span></td>
    </tr>
  </table>
  <h2>4. Optimization Strategies</h2>
  <ul>
    <li>
      <p><strong>Chunking the logit matrix</strong>: Chunking over the batch can avoid materializing the full logit matrix. The logit matrix is divided into chunks over the batch size dimension, and the cross-entropy loss is computed for each chunk. The final loss is the sum of the losses of all chunks.</p>
    </li>
    <li>
      <p><strong>Compute the gradient of logit in place</strong>: The gradient of the logit matrix is computed in place, and the gradient of the input is computed by multiplying the gradient of the logit matrix with the weight matrix.</p>
    </li>
  </ul>
  <h3>4.1. <a href="https://github.com/mgmalek/efficient_cross_entropy">efficient_cross_entropy</a></h3>
  <h3>4.2. <a href="https://github.com/linkedin/Liger-Kernel">liger kernel</a></h3>
  <h3>4.3. <a href="https://arxiv.org/pdf/2411.09009">cut your losses in large-vocabulary language models</a></h3>
</div>
 </div> <footer> <a href="/blog" class="back-link">← Back to Blog</a> </footer> </article> </div> </main> </body></html>