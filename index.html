<!DOCTYPE html><html lang="en"> <head><meta charset="utf-8"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Astro v5.10.1"><!-- SEO Meta Tags --><title>Xiaotian Han - Computer Science Professor | Machine Learning &amp; LLM Research</title><meta name="title" content="Xiaotian Han - Computer Science Professor | Machine Learning &#38; LLM Research"><meta name="description" content="Assistant Professor of Computer Science at Case Western Reserve University. Research focuses on machine learning and large language models (LLMs). PhD from Texas A&#38;M University."><meta name="keywords" content="machine learning, large language models, computer science, academic research, LLM, artificial intelligence, professor"><meta name="author" content="Xiaotian Han"><meta name="robots" content="index, follow"><meta name="language" content="en"><meta name="revisit-after" content="7 days"><!-- Canonical URL --><link rel="canonical" href="https://ahxt.github.io/"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://ahxt.github.io/"><meta property="og:title" content="Xiaotian Han - Computer Science Professor | Machine Learning &#38; LLM Research"><meta property="og:description" content="Assistant Professor of Computer Science at Case Western Reserve University. Research focuses on machine learning and large language models (LLMs). PhD from Texas A&#38;M University."><meta property="og:image" content="https://ahxt.github.io//xt.png"><meta property="og:image:alt" content="Xiaotian Han - Profile Photo"><meta property="og:site_name" content="Xiaotian Han - Academic Homepage"><meta property="og:locale" content="en_US"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://ahxt.github.io/"><meta property="twitter:title" content="Xiaotian Han - Computer Science Professor | Machine Learning &#38; LLM Research"><meta property="twitter:description" content="Assistant Professor of Computer Science at Case Western Reserve University. Research focuses on machine learning and large language models (LLMs). PhD from Texas A&#38;M University."><meta property="twitter:image" content="https://ahxt.github.io//xt.png"><meta property="twitter:image:alt" content="Xiaotian Han - Profile Photo"><meta property="twitter:creator" content="@XiaotianHan1"><!-- Additional Meta Tags for Academic Site --><meta name="theme-color" content="#1e40af"><meta name="msapplication-TileColor" content="#1e40af"><!-- Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"Xiaotian Han","jobTitle":"Assistant Professor of Computer Science","worksFor":{"@type":"Organization","name":"Case Western Reserve University","url":"https://case.edu"},"alumniOf":{"@type":"Organization","name":"Texas A&M University"},"knowsAbout":["Machine Learning","Large Language Models","Computer Science","Artificial Intelligence","LLMs"],"url":"https://ahxt.github.io/","image":"https://ahxt.github.io//xt.png","sameAs":["https://scholar.google.com/citations?hl=en&user=Uromx98AAAAJ&view_op=list_works&sortby=pubdate","https://x.com/XiaotianHan1","https://bsky.app/profile/xhan2.bsky.social","https://github.com/ahxt"]}</script><!-- Preconnect to external domains for performance --><link rel="preconnect" href="https://scholar.google.com"><link rel="preconnect" href="https://github.com"><link rel="dns-prefetch" href="https://x.com"><link rel="dns-prefetch" href="https://bsky.app"><header> <nav> <div class="logo-container"> <h2 class="logo"> <a href="/" class="logo-link"> <span class="logo-text">Xiaotian Han</span> </a> </h2> </div> <div class="internal-links"> <a href="/" class="nav-link active"> <span class="nav-text">About</span> <div class="nav-indicator"></div> </a> <a href="/blog" class="nav-link "> <span class="nav-text">Blog</span> <div class="nav-indicator"></div> </a> </div> </nav> </header> <div class="footer" hidden="hidden"> <div class="center"> <a><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=m&d=91g_Uih-7fadH9madF_Vex1LQXOVlduL5aeBBSKXgXA&co=2d78ad&ct=ffffff"></a> </div> </div><link rel="stylesheet" href="/_astro/blog.CJoC8E46.css"></head> <body> <main> <section aria-label="About Xiaotian Han" class="about-section"> <div class="about-container"> <div class="about-content"> <div>
  <p>Hi there!</p>
  <p>I am an Assistant Professor of Computer Science at Case Western Reserve University from Fall 2024. I completed my Ph.D. at Texas A&amp;M University. My research focuses on machine learning, large language models (LLMs). You can contact me via <a href="">xhan (at) case.edu</a>.</p>
  <p><a href="https://scholar.google.com/citations?hl=en&amp;user=Uromx98AAAAJ&amp;view_op=list_works&amp;sortby=pubdate">google scholar</a> / <a href="https://x.com/XiaotianHan1"><span><math><mi mathvariant="normal">ùïè</mi></math></span></a> / <a href="https://bsky.app/profile/xhan2.bsky.social">bluesky</a> / <a href="https://github.com/ahxt">github</a> / <a href="/Xiaotian_CV.pdf">cv</a></p>
</div>
 </div> <div class="about-image"> <img src="/xt.png" alt="Xiaotian Han - Assistant Professor of Computer Science" class="profile-photo" width="400" height="400" loading="eager"> </div> </div> </section> <section aria-label="News"> <div>
  <h4>üî• News</h4>
  <ul>
    <li>
      <p><strong>2025.06</strong>: I developed a <a href="https://typst.app/">Typst</a>-based homepage template and migrated my website to it! Check out <a href="https://github.com/ahxt/academic-homepage-typst">github repo</a>!</p>
    </li>
    <li>
      <p><strong>2025.05</strong>: New preprints on <a href="https://arxiv.org/abs/2504.12329">Speculative Thinking</a>, <a href="https://arxiv.org/abs/2505.17296">SELF</a>, and <a href="https://www.arxiv.org/abs/2505.17315">Longer Context, Deeper Thinking</a> form my group are out!</p>
    </li>
    <li>
      <p><strong>2025.05</strong>: Two papers on <a href="https://arxiv.org/abs/2505.19293">üíØ-LongBench</a> and <a href="https://arxiv.org/abs/2503.19878">CausalRAG</a> accepted by ACL2025 Findings!</p>
    </li>
    <li>
      <p><strong>2025.04</strong>: Invited to serve as an Area Chair for NeurIPS2025!</p>
    </li>
    <li>
      <p><strong>2025.02</strong>: New preprint on <a href="https://arxiv.org/abs/2502.13173">Thinking Preference Optimization</a> is out!</p>
    </li>
    <li>
      <p><strong>2025.02</strong>: One paper on <a href="https://openreview.net/forum?id=XYMWd1wNlf">You Only Debias Once</a> is accepted by CPAL 2025 (Proceedings Track) Oral!</p>
    </li>
    <li>
      <p><strong>2024.12</strong>: Technical report of <a href="https://arxiv.org/pdf/2412.10743">NeuralPLexer3</a> for biomolecular complex structure prediction is out!</p>
    </li>
    <li>
      <p><strong>2024.11</strong>: <a href="https://openreview.net/forum?id=koC6zyaj73">Graph Convolution‚âàMixup</a> is selected as Oral Presentation at LoG2024 TMLR Track!</p>
    </li>
    <li>
      <p><strong>2024.08</strong>: One paper on <a href="https://openreview.net/forum?id=koC6zyaj73">Understanding Graph Convolution</a> accepted by TMLR!</p>
    </li>
    <li>
      <p><strong>2024.08</strong>: Excited to join Case Western Reserve University as an assistant professor!</p>
    </li>
    <li>
      <p><strong>2024.07</strong>: I am excited to start my MLsys internship at <a href="https://www.iambic.ai/">Iambic Therapeutics</a></p>
    </li>
    <li>
      <p><strong>2024.06</strong>: <a href="https://arxiv.org/abs/2401.01325">LLM Maybe LongLM</a> has been selected as <strong>Spotlight (3.5%)</strong> at ICML2024!</p>
    </li>
    <li><details><summary style="color: rgba(74, 99, 199, 1)">More‚Ä¶</summary><ul><li><strong>2024.05</strong>: <a href="https://arxiv.org/pdf/2312.15194">One paper</a> is accepted by ACL2024!</li><li><strong>2024.04</strong>: <a href="https://arxiv.org/abs/2401.01325">LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</a> is accepted by ICML2024!</li><li><strong>2024.05</strong>: <a href="https://arxiv.org/pdf/2312.15194">One paper</a> is accepted by ACL2024!</li><li><strong>2024.04</strong>: <a href="https://arxiv.org/abs/2401.01325">LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</a> is accepted by ICML2024!</li><li><strong>2024.04</strong>: My <a href="https://huggingface.co/ahxt/LiteLlama-460M-1T">LiteLLaMa</a> has been downloaded over 230K times on HuggingFace!</li><li><strong>2024.04</strong>: Honored to receive the <a href="https://www.janestreet.com/join-jane-street/programs-and-events/grf-profiles-2024/">Jane Street Graduate Research Fellowship Award Honorable Mention</a>.</li><li><strong>2024.03</strong>: Implemented <a href="https://github.com/openai/triton">Triton</a> based flash self-extend. Please try <a href="https://github.com/datamllab/LongLM/blob/master/self_extend_patch/selfextend_flash_attn_triton.py">FlashSelfExtend</a>!</li><li><strong>2024.01</strong>: One paper on <a href="https://arxiv.org/pdf/2306.09468.pdf">Fairness Benchmark</a> accepted by ICLR2024!</li><li><strong>2024.01</strong>: Our <a href="https://arxiv.org/abs/2304.13712">Survey on LLMs</a> accepted by TKDD!</li><li><strong>2024.01</strong>: New preprint <a href="https://arxiv.org/abs/2401.01325">LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</a>!</li><li><strong>2023.12</strong>: One paper accepted by AAAI2024-SRRAI.</li><li><strong>2023.09</strong>: One paper accepted by NeurIPS2023.</li><li><strong>2023.07</strong>: üî•üî• Thrilled to release my <a href="https://huggingface.co/ahxt/LiteLlama-460M-1T">LiteLLaMa</a> on HuggingFace, try it out!</li><li><strong>2023.07</strong>: Our paper <a href="https://arxiv.org/pdf/2303.04360.pdf">LLM for Clinical Text Mining</a> accepted by <a href="https://amia.org/education-events/amia-2023-annual-symposium">AMIA2023</a>!</li><li><strong>2023.05</strong>: One paper accepted by TMLR, <a href="https://openreview.net/forum?id=LjDFIWWVVa">Retiring ‚àÜDP</a>!</li><li><strong>2023.05</strong>: Thrilled to start my internship at Amazon.</li><li><strong>2023.01</strong>: One paper accepted by ICLR2023, <a href="https://arxiv.org/pdf/2210.00102.pdf">MLPInit</a>.</li><li><strong>2022.09</strong>: Thrilled to start my internship at Meta, work with <a href="https://wqfcr.github.io/">Qifan Wang</a>.</li><li><strong>2022.07</strong>: Our Paper <a href="https://arxiv.org/pdf/2202.07179.pdf">-Mixup</a> is awarded an Outstanding Paper Award at ICML 2022!</li><li><strong>2022.05</strong>: Thrilled to start my internship at Snap Inc., work with <a href="http://nshah.net/">Neil Shah</a>.</li><li><strong>2022.05</strong>: One paper accepted by ICML2022 (Oral).</li><li><strong>2022.01</strong>: One paper accepted by ICLR2022.</li><li><strong>2022.01</strong>: One paper accepted by TheWebConf2022.</li><li><strong>2020.05</strong>: One paper accepted by RecSys2020.</li></ul></details></li>
  </ul>
</div>
 </section> <hr> <section aria-label="CV"> <div>
  <h4>Selected Publications(<a href="https://scholar.google.com/citations?hl=en&amp;user=Uromx98AAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Google Scholar</a>)</h4>
  <table>
    <tr>
      <td><strong>[<em>arXiv</em>]</strong></td>
      <td><a href="https://arxiv.org/pdf/2505.17315">Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning</a></td>
    </tr>
    <tr>
      <td></td>
      <td>Wang Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, <strong>Xiaotian Han</strong></td>
    </tr>
    <tr>
      <td></td>
      <td><em>arXiv</em>, 2025</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[<em>arXiv</em>]</strong></td>
      <td><a href="https://arxiv.org/pdf/2505.17296">SELF: Self-Extend the Context Length With Logistic Growth Function</a></td>
    </tr>
    <tr>
      <td></td>
      <td>Phat Thanh Dang, Saahil Thoppay, Wang Yang, Qifan Wang, Vipin Chaudhary, <strong>Xiaotian Han</strong></td>
    </tr>
    <tr>
      <td></td>
      <td><em>arXiv</em>, 2025</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[<em>arXiv</em>]</strong></td>
      <td><a href="https://arxiv.org/pdf/2504.12329">Speculative Thinking: Enhancing Small Model Reasoning with Large Model Guidance at Inference Time</a></td>
    </tr>
    <tr>
      <td></td>
      <td>Wang Yang, Xiang Yue, Vipin Chaudhary, <strong>Xiaotian Han</strong></td>
    </tr>
    <tr>
      <td></td>
      <td><em>arXiv</em>, 2025</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[<em>arXiv</em>]</strong></td>
      <td><a href="https://arxiv.org/pdf/2502.13173">Thinking Preference Optimization</a></td>
    </tr>
    <tr>
      <td></td>
      <td>Wang Yang, Hongye Jin, Jingfeng Yang, Vipin Chaudhary, <strong>Xiaotian Han</strong></td>
    </tr>
    <tr>
      <td></td>
      <td><em>arXiv</em>, 2025</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[<em>arXiv</em>]</strong></td>
      <td><a href="https://arxiv.org/pdf/2412.10743">NeuralPLexer3: Physio-Realistic Biomolecular Structure Prediction with Flow Models</a></td>
    </tr>
    <tr>
      <td></td>
      <td>Zhuoran Qiao*, Feizhi Ding*, Thomas Dresselhaus*, Mia A Rosenfeld*, <strong>Xiaotian Han</strong>*, Owen Howell, Aniketh Iyengar, Stephen Opalenski, Anders S Christensen, Sai Krishna Sirumalla, Frederick R Manby, Thomas F Miller III, Matthew Welborn</td>
    </tr>
    <tr>
      <td></td>
      <td><em>arXiv</em>, 2025</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[ACL2025 Findings]</strong></td>
      <td><a href="https://arxiv.org">üíØ-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?</a></td>
    </tr>
    <tr>
      <td></td>
      <td>Wang Yang, Hongye Jin, Shaochen Zhong, Song Jiang, Qifan Wang, Vipin Chaudhary, <strong>Xiaotian Han</strong></td>
    </tr>
    <tr>
      <td></td>
      <td><em>arXiv</em>, 2025</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[CPAL2025]</strong></td>
      <td><a href="https://openreview.net/pdf?id=ukwLjaGQyL">You Only Debias Once: Towards Flexible Accuracy-Fairness Trade-offs at Inference Time</a></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>Xiaotian Han</strong>, Tianlong Chen, Kaixiong Zhou, Zhimeng Jiang, Zhangyang Wang, Xia Hu</td>
    </tr>
    <tr>
      <td></td>
      <td><em>The Conference on Parsimony and Learning (CPAL)</em>, 2025</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>CPAL2025 Oral (12 in total)</strong></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[ICML2024]</strong></td>
      <td><a href="https://arxiv.org/pdf/2401.01325">LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</a></td>
    </tr>
    <tr>
      <td></td>
      <td>Hongye Jin*, <strong>Xiaotian Han</strong>*, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu</td>
    </tr>
    <tr>
      <td></td>
      <td><em>International Conference on Machine Learning (ICML)</em>, 2024</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>ICML2024 Spotlight (3.5%)</strong></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[TMLR2024]</strong></td>
      <td><a href="https://arxiv.org/pdf/2310.00183.pdf">On the Equivalence of Graph Convolution and Mixup</a></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>Xiaotian Han</strong>, Hanqing Zeng, Yu Chen, Shaoliang Nie, Jingzhou Liu, Kanika Narang, Zahra Shakeri, Karthik Abinav Sankararaman, Song Jiang, Madian Khabsa, Qifan Wang, Xia Hu</td>
    </tr>
    <tr>
      <td></td>
      <td><em>Transactions on Machine Learning Research (TMLR)</em>, 2024</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[ICLR2024]</strong></td>
      <td><a href="https://arxiv.org/pdf/2306.09468.pdf">FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods</a></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>Xiaotian Han</strong>, Jianfeng Chi, Yu Chen, Qifan Wang, Han Zhao, Na Zou, Xia Hu</td>
    </tr>
    <tr>
      <td></td>
      <td><em>International Conference on Learning Representations (ICLR)</em>, 2024</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[TKDD2024]</strong></td>
      <td><a href="https://arxiv.org/pdf/2304.13712.pdf">Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</a></td>
    </tr>
    <tr>
      <td></td>
      <td>Jingfeng Yang*, Hongye Jin*, Ruixiang Tang*, <strong>Xiaotian Han</strong>*, Qizhang Feng*, Haoming Jiang, Bing Yin, Xia Hu</td>
    </tr>
    <tr>
      <td></td>
      <td><em>Transactions on Knowledge and Data Engineering (TKDD)</em>, 2024</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[<em>arXiv</em>]</strong></td>
      <td><a href="https://arxiv.org/pdf/2310.00576.pdf">GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length</a></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>Xiaotian Han</strong>*, Hongye Jin*, Jingfeng Yang, Zhimeng Jiang, Chia-Yuan Chang, Xia Hu</td>
    </tr>
    <tr>
      <td></td>
      <td><em>arXiv</em>, 2023</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[NeurIPS2023]</strong></td>
      <td><a href="https://ahxt.github.io">Chasing Fairness under Distribution Shift: a Model Weight Perturbation Approach</a></td>
    </tr>
    <tr>
      <td></td>
      <td>Zhimeng Jiang*, <strong>Xiaotian Han</strong>*, Hongye Jin, Guanchu Wang, Rui Chen, Na Zou, Xia Hu</td>
    </tr>
    <tr>
      <td></td>
      <td><em>Neural Information Processing Systems (NeurIPS)</em>, 2023</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[ICLR2023]</strong></td>
      <td><a href="https://arxiv.org/pdf/2210.00102.pdf">MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization</a></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>Xiaotian Han</strong>, Tong Zhao, Yozen Liu, Xia Hu, Neil Shah</td>
    </tr>
    <tr>
      <td></td>
      <td><em>International Conference on Learning Representations (ICLR)</em>, 2023</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[TMLR2023]</strong></td>
      <td><a href="https://arxiv.org/pdf/2301.13443.pdf">Retiring DP: New Distribution-Level Metrics for Demographic Parity</a></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>Xiaotian Han</strong>*, Zhimeng Jiang*, Hongye Jin*, Zirui Liu, Na Zou, Qifan Wang, Xia Hu</td>
    </tr>
    <tr>
      <td></td>
      <td><em>Transactions on Machine Learning Research (TMLR)</em>, 2023</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[ICML2022]</strong></td>
      <td><a href="https://arxiv.org/pdf/2202.07179.pdf">G-Mixup: Graph Augmentation for Graph Classification</a></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>Xiaotian Han</strong>, Zhimeng Jiang, Ninghao Liu, Xia Hu</td>
    </tr>
    <tr>
      <td></td>
      <td><em>International Conference on Machine Learning (ICML)</em>, 2022</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>ICML2022 Outstanding Paper Award</strong></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[WWW2022]</strong></td>
      <td><a href="https://arxiv.org/pdf/2202.06241.pdf">Graph Representation Learning via Unsupervised Rate Reduction Maximization</a></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>Xiaotian Han</strong>, Zhimeng Jiang, Ninghao Liu, Xia Hu</td>
    </tr>
    <tr>
      <td></td>
      <td><em>The Web Conference (WWW)</em>, 2022</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><strong>[IJCAI2018]</strong></td>
      <td><a href="https://www.ijcai.org/proceedings/2018/0471.pdf">Aspect-Level Deep Collaborative Filtering via Heterogeneous Information Networks</a></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>Xiaotian Han</strong>, Chuan Shi, Senzhang Wang, Philip, S Yu, Li Song</td>
    </tr>
    <tr>
      <td></td>
      <td><em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2018</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
  </table>
  <h4>Education</h4>
  <ul>
    <li><em>Ph.D. in Computer Science</em>, Texas A&amp;M University, College Station, TXSep 2019 ‚Äì¬†¬† Aug 2024<br></li>
    <li><em>M.S. in Computer Science</em>, Beijing University of Posts and TelecommunicationsSep 2016 ‚Äì June 2019<br></li>
    <li><em>B.S. in Information Engineering</em>, Shandong UniversitySep 2011 ‚Äì June 2015<br></li>
  </ul>
  <h4>Awards &amp; Honors</h4>
  <ul>
    <li>ICML2022 Outstanding Paper Award2022<br></li>
    <li>Jane Street Graduate Research Fellowship Award Honorable Mention2024<br></li>
    <li>Excellent Ph.D. Student Award (One Per Year), Department of CSE, Texas A&amp;M University2023<br></li>
    <li>NeurIPS 2023 Scholar Award2023<br></li>
    <li>Grad School Research and Presentation Travel Award, Texas A&amp;M University2023<br></li>
    <li>Best Paper Awards, ADMA20182018</li>
    <li>Travel Grant, Department of CSE, Texas A&amp;M University2022, 2023</li>
    <li>Travel Award, ICML20222022</li>
    <li>Outstanding Reviewer Award, ICML20222022</li>
    <li>Best Reviewer Award, CCF Transactions on Pervasive Computing and Interaction2019</li>
    <li>National Graduate Scholarship, Beijing University of Posts and Telecommunications2018</li>
  </ul>
  <h4>Professional Experiences</h4>
  <p><strong>Iambic Therapeutics</strong>, San Diego, CAJuly 2024 ‚Äì Dec 2024</p>
  <ul>
    <li><em>ML System Intern</em>, Working on efficient LLM, Triton Kernel Devolvement, Protein Language Model Inference</li>
    <li>Mentor: <a href="https://zrqiao.github.io/">Zhuoran Qiao</a></li>
  </ul>
  <p><strong>Amazon</strong>, Palo Alto, CAMay 2023 ‚Äì Aug 2023</p>
  <ul>
    <li><em>Applied Scientist Intern</em>, Large Language Model Alignment</li>
    <li>Mentor: <a href="https://jingfengyang.github.io/">Jingfeng Yang</a></li>
  </ul>
  <p><strong>Meta</strong>, Menlo Park, CASept 2022 ‚Äì April 2023</p>
  <ul>
    <li><em>Research Intern</em>, Understanding graph neural networks</li>
    <li>Mentor: <a href="https://wqfcr.github.io/">Qifan Wang</a></li>
  </ul>
  <p><strong>Snap Research</strong>, Seattle, WAMay 2022 ‚Äì Aug 2022</p>
  <ul>
    <li><em>Research Intern</em>, Efficient large-scale graph neural networks</li>
    <li>Mentor: <a href="http://nshah.net/">Neil Shah</a></li>
  </ul>
  <h4>Professional Services</h4>
  <p><strong>Area Chair</strong>: NeurIPS 2025</p>
  <p><strong>Program Committee/Reviewer</strong>: ICLR 2024; WSDM 2024; CIKM 2023; ICML 2022, 2023; NeurIPS 2022, 2023, 2025; AAAI 2021, 2022, 2023, 2024; IJCAI 2021, 2023; WWW 2023; EMNLP 2023; ICDM 2022; KDD 2023; TIST 2023; TMLR 2023; TKDE 2023; TNNLS 2023; Neurocomputing 2023; TCPI 2019</p>
  <p><strong>Session Chair</strong>: WWW 2023; ICML 2022</p>
</div>
 </section> </main> </body></html>